{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mixed-precision\n",
    "\n",
    "This is an implementation of this model tweaked to use mixed precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html\n",
    "# !pip install -U tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../scripts/upgrade_env.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../scripts/upgrade_env.sh\n",
    "pip install -U --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html\n",
    "pip install -U tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing/writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../models/model_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../models/model_2.py\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "train = pd.read_csv(\"/mnt/tweet-sentiment-extraction/train.csv\")\n",
    "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
    "    # yes this really happens lol, try idx 314\n",
    "    if pd.isnull(tweet) or pd.isnull(selected_text) or len(tweet) == 0 or len(selected_text) == 0:\n",
    "        raise ValueError(\"text or selected_text is nan.\")\n",
    "    \n",
    "    # get indicial boundaries of substring.\n",
    "    target_char_idx_start = tweet.index(selected_text)\n",
    "    target_char_idx_end = target_char_idx_start + len(selected_text)\n",
    "\n",
    "    # build the character attention mask (used to build the token attention mask)\n",
    "    char_target_mask = (\n",
    "        [0] * target_char_idx_start +\n",
    "        [1] * (target_char_idx_end - target_char_idx_start) +\n",
    "        [0] * (len(tweet) - target_char_idx_end)\n",
    "    )\n",
    "    \n",
    "    # tokenize\n",
    "    # `ids` is the token values, `offsets` are the position tuples for the tokes in the str\n",
    "    tokens_obj = tokenizer.encode(tweet)\n",
    "    token_ids, token_offsets = tokens_obj.ids, tokens_obj.offsets\n",
    "    \n",
    "    # this is the clever bit. recall that the task is to find the subsequence in the sequence\n",
    "    # exemplifying the given sentiment. to do this we reformulate the input sequence as a\n",
    "    # question-answer pair, where the sentiment (as a single word) is the question and the\n",
    "    # sequence as a whole is the answer.\n",
    "    # \n",
    "    # this allows us to use version of the BERT model pretrained on the well-formed and\n",
    "    # well-studied question-answering task as a surrogate for this task.\n",
    "    sentiment_id_map = {\n",
    "        'positive': 3893,\n",
    "        'negative': 4997,\n",
    "        'neutral': 8699\n",
    "    }\n",
    "    # 101 is [CLS] and 102 is [SEP]. BERT expects Q/A input to be in the form\n",
    "    # [CLS] [...] [SEP] [...] [SEP]. Cf.\n",
    "    # https://huggingface.co/transformers/glossary.html#token-type-ids\n",
    "    # NOTE: the [-1:1] is the excise the start-of-seq and end-of-seq in the tokens\n",
    "    input_ids = [101] + [sentiment_id_map[sentiment]] + [102] + token_ids[1:-1] + [102]\n",
    "    \n",
    "    # BERT expects Q/A pairs to come with a binary mask splitting the pair types\n",
    "    # NOTE: the mafs excludes start-of-seq and end-of-seq but includes the new end-of-seq\n",
    "    token_type_ids = [0, 0, 0] + [1] * (len(token_ids) - 2 + 1)\n",
    "\n",
    "    # pad to max_len and create a corresponding attention mask\n",
    "    pad_len = max_len - len(input_ids)\n",
    "    attention_mask = [1] * len(input_ids) + [0] * pad_len\n",
    "    input_ids = input_ids + [0] * pad_len\n",
    "    token_type_ids = token_type_ids + [0] * pad_len\n",
    "    \n",
    "    # get the index of the first and last token of the target, this is what the model will try\n",
    "    # to predict! see the notes on the head layer in forward for more info.\n",
    "    # we add 3 because the first thee elements of the mask are always [CLS] $SENTIMENT [CLS]\n",
    "    # and always get an attention vector [1 1 1].\n",
    "    ufunc = lambda first, _: first >= target_char_idx_start and first < target_char_idx_end\n",
    "    y_pred_mask = [ufunc(*offset) for offset in token_offsets]\n",
    "    try:\n",
    "        y_first = 3 + y_pred_mask.index(True)\n",
    "        y_last = 3 + len(y_pred_mask) - y_pred_mask[::-1].index(True) - 1\n",
    "    except ValueError:\n",
    "        # some of the labels are noisy, and the first character in the label does not actually\n",
    "        # correspond with the first character of any token (e.g. the label is a part-of-a-word\n",
    "        # instead of a word). I'm going to venture the opinion here that these records \n",
    "        # constitute data noise (because, I mean, they are) and should be removed in\n",
    "        # pre-processing\n",
    "        raise ValueError(\n",
    "            f\"Found bad selected_text value '{selected_text}' for tweet '{tweet}'.\"\n",
    "            f\"Make sure to get rid of these in a pre-processing pass.\"\n",
    "        )\n",
    "    \n",
    "    # convert to torch tensors\n",
    "    t = lambda seq: torch.tensor(seq, dtype=torch.long)\n",
    "    input_ids, token_type_ids, attention_mask, y_first, y_last =\\\n",
    "        t(input_ids), t(token_type_ids), t(attention_mask), t(y_first), t(y_last)\n",
    "    \n",
    "    # output\n",
    "    # Unfortunately the PyTorch dataloader relies on pickle, and TIL namedtuples do not play nice\n",
    "    # with pickle!\n",
    "    # Record = namedtuple('record', 'input_ids token_type_ids attention_mask y_first y_last')\n",
    "    # return Record(input_ids, token_type_ids, attention_mask, y_first, y_last)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"y_first\": y_first,\n",
    "        \"y_last\": y_last\n",
    "    }\n",
    "\n",
    "class TwitterSentimentExtractionDataset:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizers.BertWordPieceTokenizer(\n",
    "            f\"/mnt/bert-base-uncased/vocab.txt\", lowercase=True\n",
    "        )\n",
    "        self.max_len = 128\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return process_data(\n",
    "            self.df.text[item],\n",
    "            self.df.selected_text[item], \n",
    "            self.df.sentiment[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "        )\n",
    "\n",
    "# preprocessing pass; see comments in the previous code cell on why this is necessary\n",
    "X_train_preprocessing_pass = TwitterSentimentExtractionDataset(train)\n",
    "\n",
    "bad_idxs, good_idxs, y_firsts, y_lasts = [], [], [], []\n",
    "for i in range(len(train)):\n",
    "# for i in tqdm.tqdm(list(range(len(train)))):\n",
    "    try:\n",
    "        x = X_train_preprocessing_pass[i]\n",
    "        y_firsts.append(x['y_first'])\n",
    "        y_lasts.append(x['y_last'])\n",
    "        good_idxs.append(i)\n",
    "    except ValueError:\n",
    "        print(f\"Found bad record at idx {i}.\")\n",
    "        y_firsts.append(None)\n",
    "        y_lasts.append(None)\n",
    "        bad_idxs.append(i)\n",
    "\n",
    "del X_train_preprocessing_pass\n",
    "train_orig = train\n",
    "X_train_df = train.iloc[good_idxs].reset_index(drop=True)\n",
    "\n",
    "class TwitterSentimentExtractionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # configuring the model to output hidden states\n",
    "        cfg = transformers.PretrainedConfig.get_config_dict(\"bert-base-uncased\")[0]  # tuple?\n",
    "        cfg[\"output_hidden_states\"] = True\n",
    "        cfg = transformers.BertConfig.from_dict(cfg)\n",
    "        self.bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\", config=cfg)\n",
    "        \n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.l0 = nn.Linear(768 * 2, 2)\n",
    "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
    "        # self.out = nn.LogSoftmax(dim=-2)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # ignore the output from the model head, we'll instead we'll construct our own attention\n",
    "        # head connected to the last two layers of hidden weights.\n",
    "        # that's 512x762x2=780288 connections.\n",
    "        _, _, out = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        out = torch.cat((out[-1], out[-2]), dim=-1)\n",
    "        out = self.drop_out(out)\n",
    "        # the new head uses a linear layer with two output nodes.\n",
    "        # the first node learns sequence start.\n",
    "        # the second node learns sequence end.\n",
    "        logits = self.l0(out)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        \n",
    "        return start_logits, end_logits\n",
    "        \n",
    "        # TODO: embed softmax directly into the model arch\n",
    "        # y_start, y_end = self.out(start_logits), self.out(end_logits)\n",
    "        # return y_start, y_end\n",
    "\n",
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    y_pred_first_loss = ce(start_logits, start_positions)\n",
    "    y_pred_last_loss = ce(end_logits, end_positions)\n",
    "    y_pred_loss = (y_pred_first_loss + y_pred_last_loss)\n",
    "    return y_pred_loss\n",
    "\n",
    "# create model\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = TwitterSentimentExtractionModel()\n",
    "model.to(device)\n",
    "\n",
    "# create optimizer; uses weight decay\n",
    "model_parameters = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "    {'params': [p for n, p in model_parameters if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "    {'params': [p for n, p in model_parameters if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "\n",
    "# batch size and epochs globals; setting batch_size = 1 for testing, 64 is too much for the K80\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "# dataset and dataloader\n",
    "dataset = TwitterSentimentExtractionDataset(X_train_df)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "# create scheduler, this one is a transformers module hookup, thanks huggingface\n",
    "num_train_steps = int(len(dataloader) * epochs)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_train_steps\n",
    ")\n",
    "\n",
    "# tensorboard writer\n",
    "writer = SummaryWriter(f'/spell/tensorboards/model_2')\n",
    "\n",
    "# mixed-precision gradient scaler\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# train func for one epoch of training\n",
    "def train_fn(dataloader, model, optimizer, device, scheduler, epoch_num):\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    fn = lambda field: records[field].to(device, dtype=torch.long)\n",
    "    for idx, records in enumerate(dataloader):\n",
    "        # move the record to GPU\n",
    "        input_ids = fn(\"input_ids\")\n",
    "        token_type_ids = fn(\"token_type_ids\")\n",
    "        attention_mask = fn(\"attention_mask\")\n",
    "        y_first = fn(\"y_first\")\n",
    "        y_last = fn(\"y_last\")\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        # mixed-precision autocast\n",
    "        with torch.cuda.amp.autocast():        \n",
    "            y_pred_start_logits, y_pred_end_logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "            )\n",
    "            loss = loss_fn(y_pred_start_logits, y_pred_end_logits, y_first, y_last)\n",
    "            y_pred_starts = torch.softmax(y_pred_start_logits, dim=1).cpu().detach().numpy()\n",
    "            y_pred_ends = torch.softmax(y_pred_end_logits, dim=1).cpu().detach().numpy()\n",
    "        \n",
    "        # mixed-precision scaling\n",
    "        scaler.scale(loss).backward()        \n",
    "        # loss.backward()\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        lv = loss.item()\n",
    "        losses.append(lv)\n",
    "        writer.add_scalar('training loss', lv, epoch_num * len(dataloader) + idx)\n",
    "        \n",
    "        if idx % 10 == 0:\n",
    "            print(f\"epoch {epoch_num}, batch {idx} training loss: {losses[-1]}\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "def main():\n",
    "    checkpoints_dir = \"/spell/checkpoints/\"\n",
    "    if not os.path.exists(checkpoints_dir):\n",
    "        os.mkdir(checkpoints_dir)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        losses = train_fn(dataloader, model, optimizer, device, scheduler, epoch)\n",
    "        avg_loss = np.mean(losses)\n",
    "        print(f\"epoch {epoch}, average training loss: {avg_loss}\")\n",
    "        torch.save(model.state_dict(), f\"/spell/checkpoints/model_{epoch}.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when you run this locally you will see the following warning:\n",
    "\n",
    "```\n",
    "Found bad record at idx 18.\n",
    "Found bad record at idx 314.\n",
    "Found bad record at idx 5436.\n",
    "Found bad record at idx 8729.\n",
    "Found bad record at idx 11808.\n",
    "Found bad record at idx 12405.\n",
    "Found bad record at idx 14172.\n",
    "Found bad record at idx 15207.\n",
    "Found bad record at idx 21983.\n",
    "Found bad record at idx 24929.\n",
    "epoch 1, batch 0 training loss: 10.138711929321289\n",
    "\n",
    "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
    "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
    "\n",
    "epoch 1, batch 10 training loss: 6.6914567947387695\n",
    "epoch 1, batch 20 training loss: 5.71492862701416\n",
    "epoch 1, batch 30 training loss: 4.4501872062683105\n",
    "epoch 1, batch 40 training loss: 3.9177305698394775\n",
    "epoch 1, batch 50 training loss: 3.7678070068359375\n",
    "epoch 1, batch 60 training loss: 3.488056182861328\n",
    "epoch 1, batch 70 training loss: 3.548919677734375\n",
    "epoch 1, batch 80 training loss: 3.2360477447509766\n",
    "```\n",
    "\n",
    "This occurs because the exponential backoff built into the loss autoscaler will drop an update on the floor, causing `scheduler.step()` to get called twice consequetively with no `optimizer.step()` in between. The suggested help is not actually relevant because it is not the issue. In this scenario it is safe to silence the message (e.g. using the Python `warnings` module)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
