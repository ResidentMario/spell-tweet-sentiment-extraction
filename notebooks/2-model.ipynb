{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initial-model\n",
    "\n",
    "This notebook is the prototype build for the initial model. The intent is getting something working; there may be some not-niceness involved which will be cleaned up in future iterations.\n",
    "\n",
    "Credit where credit is due, this model is a rewrite of [BERT Base Uncased using PyTorch](https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"/mnt/tweet-sentiment-extraction/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>, don`t force</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>Yay good for both of you.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>All this flirting going on - The ATG smiles. Y...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "0      cb774db0d1                I`d have responded, if I were going   \n",
       "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2      088c60f138                          my boss is bullying me...   \n",
       "3      9642c003ef                     what interview! leave me alone   \n",
       "4      358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "...           ...                                                ...   \n",
       "27476  4eac33d1c0   wish we could come see u on Denver  husband l...   \n",
       "27477  4f4c4fc327   I`ve wondered about rake to.  The client has ...   \n",
       "27478  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n",
       "27479  ed167662a5                         But it was worth it  ****.   \n",
       "27480  6f7127d9d7     All this flirting going on - The ATG smiles...   \n",
       "\n",
       "                                           selected_text sentiment  \n",
       "0                    I`d have responded, if I were going   neutral  \n",
       "1                                               Sooo SAD  negative  \n",
       "2                                            bullying me  negative  \n",
       "3                                         leave me alone  negative  \n",
       "4                                          Sons of ****,  negative  \n",
       "...                                                  ...       ...  \n",
       "27476                                             d lost  negative  \n",
       "27477                                      , don`t force  negative  \n",
       "27478                          Yay good for both of you.  positive  \n",
       "27479                         But it was worth it  ****.  positive  \n",
       "27480  All this flirting going on - The ATG smiles. Y...   neutral  \n",
       "\n",
       "[27481 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on some lessons learned during the replication process.\n",
    "\n",
    "1. > You're attempting to predict the word or phrase from the tweet that exemplifies the provided sentiment.\n",
    "  \n",
    "  So `text` and `sentiment` are inputs, `selected_text` is the target. `selected_text` is a sub-sentence fragment.\n",
    "  \n",
    "  It's not immediately obvious to me how you would modify the design of the neural network to achieve this, so this is a good exercise in network architecture imagination besides.\n",
    "2. There are two interfaces to language encoders in the `huggingface` ecosystem: `tokenizers`, the low-level library, and `transformers`, the original library.\n",
    "  \n",
    "  The `transformers` interface lets you instantiate a pretrained network without having to download the dataset, but does not offer an interface to any of the methods besides the output tokens themselves. \n",
    "  \n",
    "  The `tokenizers` interface forces you to have the whole thing on disk, but exposes a richer underlying API. This model uses this underlying API, so we will have to do things this way.\n",
    "3. `transformers` models let you swap to the version of the BERT model trained (fine-tuned?) on question-answer pairs transparently by setting the `token_type_ids` property on the model. We're adapting question-answering to this task by cleverly encoding the input records as a `sentiment:tweet` pair and using a custom head for outputting logits for each element of the output sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prototype data part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from tqdm.autonotebook import tqdm\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
    "    # yes this really happens lol, try idx 314\n",
    "    if pd.isnull(tweet) or pd.isnull(selected_text) or len(tweet) == 0 or len(selected_text) == 0:\n",
    "        raise ValueError(\"text or selected_text is nan.\")\n",
    "    \n",
    "    # get indicial boundaries of substring.\n",
    "    target_char_idx_start = tweet.index(selected_text)\n",
    "    target_char_idx_end = target_char_idx_start + len(selected_text)\n",
    "\n",
    "    # build the character attention mask (used to build the token attention mask)\n",
    "    char_target_mask = (\n",
    "        [0] * target_char_idx_start +\n",
    "        [1] * (target_char_idx_end - target_char_idx_start) +\n",
    "        [0] * (len(tweet) - target_char_idx_end)\n",
    "    )\n",
    "    \n",
    "    # tokenize\n",
    "    # `ids` is the token values, `offsets` are the position tuples for the tokes in the str\n",
    "    tokens_obj = tokenizer.encode(tweet)\n",
    "    token_ids, token_offsets = tokens_obj.ids, tokens_obj.offsets\n",
    "    \n",
    "    # this is the clever bit. recall that the task is to find the subsequence in the sequence\n",
    "    # exemplifying the given sentiment. to do this we reformulate the input sequence as a\n",
    "    # question-answer pair, where the sentiment (as a single word) is the question and the\n",
    "    # sequence as a whole is the answer.\n",
    "    # \n",
    "    # this allows us to use version of the BERT model pretrained on the well-formed and\n",
    "    # well-studied question-answering task as a surrogate for this task.\n",
    "    sentiment_id_map = {\n",
    "        'positive': 3893,\n",
    "        'negative': 4997,\n",
    "        'neutral': 8699\n",
    "    }\n",
    "    # 101 is [CLS] and 102 is [SEP]. BERT expects Q/A input to be in the form\n",
    "    # [CLS] [...] [SEP] [...] [SEP]. Cf.\n",
    "    # https://huggingface.co/transformers/glossary.html#token-type-ids\n",
    "    # NOTE: the [-1:1] is the excise the start-of-seq and end-of-seq in the tokens\n",
    "    input_ids = [101] + [sentiment_id_map[sentiment]] + [102] + token_ids[1:-1] + [102]\n",
    "    \n",
    "    # BERT expects Q/A pairs to come with a binary mask splitting the pair types\n",
    "    # NOTE: the mafs excludes start-of-seq and end-of-seq but includes the new end-of-seq\n",
    "    token_type_ids = [0, 0, 0] + [1] * (len(token_ids) - 2 + 1)\n",
    "\n",
    "    # pad to max_len and create a corresponding attention mask\n",
    "    pad_len = max_len - len(input_ids)\n",
    "    attention_mask = [1] * len(input_ids) + [0] * pad_len\n",
    "    input_ids = input_ids + [0] * pad_len\n",
    "    token_type_ids = token_type_ids + [0] * pad_len\n",
    "    \n",
    "    # get the index of the first and last token of the target, this is what the model will try\n",
    "    # to predict! see the notes on the head layer in forward for more info.\n",
    "    # we add 3 because the first thee elements of the mask are always [CLS] $SENTIMENT [CLS]\n",
    "    # and always get an attention vector [1 1 1].\n",
    "    ufunc = lambda first, _: first >= target_char_idx_start and first < target_char_idx_end\n",
    "    y_pred_mask = [ufunc(*offset) for offset in token_offsets]\n",
    "    try:\n",
    "        y_first = 3 + y_pred_mask.index(True)\n",
    "        y_last = 3 + len(y_pred_mask) - y_pred_mask[::-1].index(True) - 1\n",
    "    except ValueError:\n",
    "        # some of the labels are noisy, and the first character in the label does not actually\n",
    "        # correspond with the first character of any token (e.g. the label is a part-of-a-word\n",
    "        # instead of a word). I'm going to venture the opinion here that these records \n",
    "        # constitute data noise (because, I mean, they are) and should be removed in\n",
    "        # pre-processing\n",
    "        raise ValueError(\n",
    "            f\"Found bad selected_text value '{selected_text}' for tweet '{tweet}'.\"\n",
    "            f\"Make sure to get rid of these in a pre-processing pass.\"\n",
    "        )\n",
    "    \n",
    "    # convert to torch tensors\n",
    "    t = lambda seq: torch.tensor(seq, dtype=torch.long)\n",
    "    input_ids, token_type_ids, attention_mask, y_first, y_last =\\\n",
    "        t(input_ids), t(token_type_ids), t(attention_mask), t(y_first), t(y_last)\n",
    "    \n",
    "    # output\n",
    "    # Unfortunately the PyTorch dataloader relies on pickle, and TIL namedtuples do not play nice\n",
    "    # with pickle!\n",
    "    # Record = namedtuple('record', 'input_ids token_type_ids attention_mask y_first y_last')\n",
    "    # return Record(input_ids, token_type_ids, attention_mask, y_first, y_last)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"y_first\": y_first,\n",
    "        \"y_last\": y_last\n",
    "    }\n",
    "\n",
    "class TwitterSentimentExtractionDataset:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizers.BertWordPieceTokenizer(\n",
    "            f\"/mnt/bert-base-uncased/vocab.txt\", lowercase=True\n",
    "        )\n",
    "        self.max_len = 128\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return process_data(\n",
    "            self.df.text[item],\n",
    "            self.df.selected_text[item], \n",
    "            self.df.sentiment[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 823/27481 [00:00<00:06, 4095.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found bad record at idx 18.\n",
      "Found bad record at idx 314.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 5875/27481 [00:01<00:05, 4136.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found bad record at idx 5436.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 9188/27481 [00:02<00:04, 4102.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found bad record at idx 8729.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 12582/27481 [00:03<00:03, 4199.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found bad record at idx 11808.\n",
      "Found bad record at idx 12405.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 14702/27481 [00:03<00:03, 4232.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found bad record at idx 14172.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 15976/27481 [00:03<00:02, 4193.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found bad record at idx 15207.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 22764/27481 [00:05<00:01, 3853.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found bad record at idx 21983.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 25698/27481 [00:06<00:00, 4150.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found bad record at idx 24929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27481/27481 [00:06<00:00, 4118.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing pass; see comments in the previous code cell on why this is necessary\n",
    "X_train_preprocessing_pass = TwitterSentimentExtractionDataset(train)\n",
    "\n",
    "import tqdm\n",
    "bad_idxs, good_idxs, y_firsts, y_lasts = [], [], [], []\n",
    "for i in tqdm.tqdm(list(range(len(train)))):\n",
    "    try:\n",
    "        x = X_train_preprocessing_pass[i]\n",
    "        y_firsts.append(x['y_first'])\n",
    "        y_lasts.append(x['y_last'])\n",
    "        good_idxs.append(i)\n",
    "    except ValueError:\n",
    "        print(f\"Found bad record at idx {i}.\")\n",
    "        y_firsts.append(None)\n",
    "        y_lasts.append(None)\n",
    "        bad_idxs.append(i)\n",
    "\n",
    "del X_train_preprocessing_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f974684b3d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAa+0lEQVR4nO3deZCd1Z3e8e/v7n1vb2qp1S200MKI1WNsRoPl2BMvhMU2HkiV7VDlRTXBRaWKTDzxpCb2JFVUPCEZp6aM7VTGUxQQ47HHmAJ7IA5lmwEmiRODLRabRQKJRaBGSwtJ3er1br/88Z571RJaWqjVt3XP86nquu8973tvn6Pqet6j8573PebuiIhIHFKtroCIiCwchb6ISEQU+iIiEVHoi4hERKEvIhKRTKsrcDzLli3zoaGhVldDROSM8sQTT+x19/6j7VvUoT80NMSmTZtaXQ0RkTOKmW0/1j4N74iIREShLyISEYW+iEhEFPoiIhFR6IuIREShLyISEYW+iEhE2jL0hw9M8fWfv8CreydaXRURkUWlLUP/wGSZbz2yjc07x1pdFRGRRaUtQ39FTwcAO0enW1wTEZHFpS1Df0kxSy6TYveYQl9EZLa2DH0zY7C7oJ6+iMgR2jL0AQa7C+xST19E5DBtG/oDPQUN74iIHKFtQ39FTzK84+6troqIyKLRtqE/0F2gXK1zYLLS6qqIiCwabRv6K3oKgKZtiojM1rahP9CdhL7G9UVEDmnb0B8MPX3N4BEROaRtQ395Vx4zDe+IiMzWtqGfTadY1plnt0JfRKSpbUMfwrRNDe+IiDS1degPdBfU0xcRmaWtQ1+PYhAROVx7h35PgdGpClPlWqurIiKyKLR36Hdr2qaIyGxtHfqNu3J3aVxfRARo89AfaN6gNdXimoiILA5tHfrN4Z3RmRbXRERkcWjr0C/lM3QVMuwaVU9fRATaPPRB0zZFRGabU+ib2b82s+fM7Fkz+4GZFcxsrZk9bmbbzOyHZpYLx+bD+21h/9Cs7/lKKH/BzK46PU063GBPgV1jGt4REYE5hL6ZrQT+FbDe3d8JpIHrga8Bt7r7ucB+4IbwkRuA/aH81nAcZnZR+NzFwNXAX5lZen6b81aD3QUN74iIBHMd3skAHWaWAYrATuAjwL1h/13AdWH72vCesP9yM7NQfre7z7j7K8A24LJTb8LxDfYUGDk4Q7VWP92/SkRk0Tth6Lv7MPCXwGskYT8KPAEccPdqOGwHsDJsrwReD5+thuOXzi4/ymdOm8GeAnWHkXEN8YiIzGV4ZwlJL30tcBZQIhmeOS3M7EYz22Rmm0ZGRk75+w5N29TFXBGRuQzv/BPgFXcfcfcK8CPg/UBvGO4BWAUMh+1hYDVA2N8DvDm7/CifaXL329x9vbuv7+/vfxtNOlxjBS0tmygiMrfQfw3YYGbFMDZ/OfA88CjwyXDMRuD+sP1AeE/Y/4i7eyi/PszuWQusA341P804tkZPXytoiYgkF2iPy90fN7N7gSeBKvAUcBvwP4G7zew/hrI7wkfuAP7GzLYB+0hm7ODuz5nZPSQnjCpwk7uf9sdf9pVy5NIpzdUXEWEOoQ/g7jcDNx9R/DJHmX3j7tPAp47xPbcAt5xkHU+JmTHQk9eYvogIEdyRC425+gp9EZE4Qr+nQxdyRUSIJfS78+wcnSa5niwiEq8oQn+gu8BMtc7oVKXVVRERaakoQn9FTwegaZsiIlGE/mBPHtBauSIiUYT+QLhBa7d6+iISuShCf3lXATMN74iIRBH6uUyKpaW8pm2KSPSiCH2AFT1aNlFEJJrQH9BduSIi8YT+YE9ePX0RiV40ob+ip4MDkxWmK6f9wZ4iIotWNKE/oBW0RETiCf0VYQUtDfGISMyiCX319EVEIgr9QfX0RUTiCf3OfIaufEY9fRGJWjShDzDQo7n6IhK3qEJfd+WKSOyiCn3dlSsisYsq9Ae7C4yMz1Ct1VtdFRGRlogr9HsK1OrO3vFyq6siItIScYV+t6Ztikjcogr9vs4cAAcm1dMXkThFFfqlXAaAiRk9dE1E4hRV6BdzaQAmytUW10REpDWiCv1SPunpT84o9EUkTlGF/qGevoZ3RCROUYV+PpMikzImNbwjIpGKKvTNjGIurQu5IhKtqEIfknF99fRFJFbRhX4xl9aYvohEK7rQL+UzTGj2johEak6hb2a9ZnavmW0xs81m9j4z6zOzh8xsa3hdEo41M/uWmW0zs9+a2aWzvmdjOH6rmW08XY06nmIuzaTG9EUkUnPt6X8T+Km7XwBcAmwGvgw87O7rgIfDe4CPAuvCz43AtwHMrA+4GXgvcBlwc+NEsZBKuYxuzhKRaJ0w9M2sB/jHwB0A7l529wPAtcBd4bC7gOvC9rXAdz3xGNBrZiuAq4CH3H2fu+8HHgKuntfWzEFyIVc9fRGJ01x6+muBEeC/m9lTZna7mZWAAXffGY7ZBQyE7ZXA67M+vyOUHav8MGZ2o5ltMrNNIyMjJ9eaOSjl0xrTF5FozSX0M8ClwLfd/T3ABIeGcgBwdwd8Pirk7re5+3p3X9/f3z8fX3mYYk49fRGJ11xCfweww90fD+/vJTkJ7A7DNoTXPWH/MLB61udXhbJjlS+oUi7NRLlKcp4SEYnLCUPf3XcBr5vZ+aHocuB54AGgMQNnI3B/2H4A+HyYxbMBGA3DQD8DrjSzJeEC7pWhbEEV8xncYaqi3r6IxCczx+P+CPi+meWAl4E/JDlh3GNmNwDbgU+HYx8EPgZsAybDsbj7PjP7c+DX4bivuvu+eWnFSSg1Hro2U6OYm2vzRUTaw5xSz92fBtYfZdflRznWgZuO8T13AneeTAXnWyPok0cx5FtZFRGRBRfhHbmHevoiIrGJMPRn9/RFROISXeg3hnf00DURiVF0od8Y3tGSiSISo/hCXz19EYlYdKHfXCdXPX0RiVB0od+4kKsnbYpIjKIL/XwmRcrQM/VFJErRhb6ZJatnqacvIhGKLvQhuZirnr6IxCjK0C/m0+rpi0iUogz9kp6pLyKRijL0izmtniUicYoy9HUhV0RiFWXoF3NpXcgVkShFGfqlnHr6IhKnOEM/rymbIhKnSENfi6OLSJyiDP1iLkPdYaZab3VVREQWVJShf2jJRI3ri0hcogz95upZGtcXkchEGfqlxjP1NYNHRCITZegXtTi6iEQqytDvbI7pa3hHROISZeg3xvTV0xeR2EQZ+iVdyBWRSEUZ+sUwvKOevojEJsrQb/b09Ux9EYlMlKFfyKYw081ZIhKfKEPfzJInbWpMX0QiE2XoQ3imvsb0RSQy0YZ+Zz6jMX0RiU60oV/Mp5nUmL6IRCbe0NfqWSISoTmHvpmlzewpM/tJeL/WzB43s21m9kMzy4XyfHi/LewfmvUdXwnlL5jZVfPdmJNRyqWZ1PCOiETmZHr6XwQ2z3r/NeBWdz8X2A/cEMpvAPaH8lvDcZjZRcD1wMXA1cBfmVn61Kr/9hXzGU3ZFJHozCn0zWwV8HHg9vDegI8A94ZD7gKuC9vXhveE/ZeH468F7nb3GXd/BdgGXDYfjXg7Srm0pmyKSHTm2tP/BvCnQGN9waXAAXdvdJV3ACvD9krgdYCwfzQc3yw/ymeazOxGM9tkZptGRkZOoiknR2P6IhKjE4a+mV0D7HH3JxagPrj7be6+3t3X9/f3n7bf05nPMFmuaXF0EYlKZg7HvB/4AzP7GFAAuoFvAr1mlgm9+VXAcDh+GFgN7DCzDNADvDmrvGH2ZxZcMZ+mVndmqnUK2ZZdWhARWVAn7Om7+1fcfZW7D5FciH3E3T8DPAp8Mhy2Ebg/bD8Q3hP2P+JJd/oB4Powu2ctsA741by15CSVms/U17i+iMTjVObp/1vgS2a2jWTM/o5QfgewNJR/CfgygLs/B9wDPA/8FLjJ3VuWuMXGOrmawSMiEZnL8E6Tu/8D8A9h+2WOMvvG3aeBTx3j87cAt5xsJU+HUl49fRGJT8R35CY9/XH19EUkItGG/qGevkJfROIRbegfGtPX8I6IxCPa0O9UT19EIhRt6Be1Tq6IRCja0C/lk+EdPVNfRGISbegXMulkcXT19EUkItGGfiplFLNaPUtE4hJt6EN4pr4u5IpIRKIOfT1TX0RiE3fo5zOasikiUYk79HMZ9fRFJCpRh34xn1ZPX0SiEnXol3IZTdkUkahEHfrFnKZsikhcog79Ul49fRGJS9ShX8ylmZipanF0EYlG1KFfymeo1p1yrd7qqoiILIi4Qz/XeOiahnhEJA5Rh34x33i8si7mikgcog79Uk6Lo4tIXKIO/WK+sWSievoiEoeoQ189fRGJTdSh31gcfVw9fRGJRNShX9Li6CISmchDvzGmr+EdEYlD3KGfU09fROISdeh3ZNXTF5G4RB36qZQlT9pUT19EIhF16AMU9Ux9EYlI9KFfyuuZ+iISj+hDv5jLMK4xfRGJRPSh36l1ckUkItGHvsb0RSQmJwx9M1ttZo+a2fNm9pyZfTGU95nZQ2a2NbwuCeVmZt8ys21m9lszu3TWd20Mx281s42nr1lzpzF9EYnJXHr6VeBP3P0iYANwk5ldBHwZeNjd1wEPh/cAHwXWhZ8bgW9DcpIAbgbeC1wG3Nw4UbRSMZfRA9dEJBonDH133+nuT4btg8BmYCVwLXBXOOwu4LqwfS3wXU88BvSa2QrgKuAhd9/n7vuBh4Cr57U1b0Mpl9YiKiISjZMa0zezIeA9wOPAgLvvDLt2AQNheyXw+qyP7Qhlxyo/8nfcaGabzGzTyMjIyVTvbSnmM1ouUUSiMefQN7NO4D7gj919bPY+d3fA56NC7n6bu6939/X9/f3z8ZXHVcqlKdfqlKtaHF1E2t+cQt/MsiSB/313/1Eo3h2GbQive0L5MLB61sdXhbJjlbdUUQ9dE5GIzGX2jgF3AJvd/euzdj0ANGbgbATun1X++TCLZwMwGoaBfgZcaWZLwgXcK0NZS3U2F0fXEI+ItL/MHI55P/A54BkzezqU/RnwF8A9ZnYDsB34dNj3IPAxYBswCfwhgLvvM7M/B34djvuqu++bl1acgsY6uZq2KSIxOGHou/svADvG7suPcrwDNx3ju+4E7jyZCp5ujWfqq6cvIjHQHbk59fRFJB7Rh35JY/oiEpHoQ7/Z09fsHRGJQPSh3+jpj2t4R0QioNAPoa+7ckUkBtGHfnNxdA3viEgEog/9dMroyKb1pE0RiUL0oQ/JM/UnNKYvIhFQ6KNn6otIPBT6JNM21dMXkRgo9Elm8KinLyIxUOiT9PQ1T19EYqDQJ3m8su7IFZEYKPRJLuRO6OYsEYmAQp9kyqZ6+iISA4U+oaevC7kiEoG5rJzV9kq5NOVqnUqtTjb91vPg2HSFbXvG2bZnnJf2jLN1zzj7Jsp87HcG+dTvrmZJKdeCWouInDyFPlBsPHStXKOnIwn98Zkq3/m/r/C3j7/GG6PTzWNz6RTn9JfIZVL8pwe38Jc/f5FPvOssPrthDe9e3UuypLCIyOKk0Cfp6UPyTP1cOsV3f/kqf/2/XmL/ZIUPnd/P5943xLnLO1m3vJPVfUXSqSTYt+wa43uPbefHTw5z35M7eOfKbj6/YYh/eunKo/6PQUSk1SxZ0nZxWr9+vW/atOm0/57/8Zs3+KMfPMUXPrCWv3v6DfaOz/DB8/r50hXnccnq3hN+fnymyo+fGub7j21ny66DvKO/xL/7+IV8+Pzl6vmLyIIzsyfcff3R9qmnTzJ7B+D2X7zC+85Zyl9/9lLWD/XN+fOd+Qyf23A2n33vGv5+8x7+84Ob+eff2cQHzl3Gv7/mQi4Y7D7mZ+t1J5XSiUFEFoZ6+sDB6Qr/5acv8NF3DvKPzl12yt9XqdX53mPb+cbfb+XgdIV/9nur+RcffAd7x8u8uPsgL+w6yIu7k5+JmRqf/N1V3PCBtQwtK81Da0Qkdsfr6Sv0T6MDk2X+6yPb+O4vX6VSO/TvXMylOW+gi/MHuqjU6/zkNzup1OtcceEAX/j9c/i9oSUaFhKRt02h32Kv7J3g0S17OHtpkfMGuljZ23HYkM6eg9P8zS+3873HtrN/ssIlq3r4zIazOX+gizV9RXqLWZ0ERGTOFPpniKlyjfue3MEdv3iFV/ZONMs78xlW9xVZ09fB0NIS5y7v5LyBLtYNdFLM6bKMiBxOoX+GqdedrXvGeW3fJK/tm+T18Pravklee3OScq3ePHZ1XwfnLe9isKfAxEyVg9PJz9h0hfGZKqVchqsuHuATl5zFuoGuFrZKRBaKQr+NVGt1tu+bZOvug7y4e5wXdx9k6+5x9hycprOQoSufpauQoauQvO4cneLxV/bhDucPdHHNu1ZwzSVnsVYXjUXalkI/cnvGpnnwmZ385Lc72bR9PwAregrkMynSKQs/KTIpo6cjy7qBTs4f6OL8wS7WDXTRmdcQksiZRKEvTW8cmOLBZ3by/BtjVOtOzZ1azZPtep03J8ps3T3OVOXQA+hWLelg7bISZ/V0sKK3wIqeAoM9HZzVU6CQTXNwusr4TJWD05UwvFRhslyjXK1TrtWZqdYpV5PXno4sF67o4oLBbs7pL+nOZZHTQDdnSdNZvR184ffPOe4x9bqzY/8UL4R7CbbsOshrb06wZddB9o7PcDL9BLPkeUX5TIpcJs3oVLk5fTWbNs5d3sWFg11cuKKbd67s4eKV3XQXsqfSRBE5DoW+vEUqZaxZWmTN0iJXXDRw2L5ytc7usWl2jk6zc3SKmUqdrkImuZ5QaFxPyFDMZchnkiGj2dNNK7U6L49MsGXXGJt3HmTLrjH+30tv8qOnhpvHDC0tcvHKHn5nZQ9DS0ss7czRV8qxtJSju5DVHcwip0DDO7Io7B2f4bk3xnh2eJRnh0d5ZniUHfun3nJcOmUsKeZY0VNgTV8xTGU99FPKp6m5U68TXh136OvM6dqEREPDO7LoLevM88Hz+vngef3NsgOTZYYPTLFvosyb42XenCizb2KGN8fLvDE6zfM7x/j587sOu9v5eFb0FHhHfyfv6E/udTinv5NCNs1MtcZMtc5Mpd7cLmTT9HRk6e3IJq/FLF2FbPMJqyJnKoW+LFq9xRy9xeMvUFOrO7vHppv3MUxXaqTMSJmRTkHKkuGl3WPTvLRnnJdGxrnvyWHGZ05+eUwz6OnINoea+ko5+kp5lpZyLCnl6Ctl6S3mWFLM0VfM0VvKUsymyehitSwiCx76ZnY18E0gDdzu7n+x0HWQ9pFOGWf1dnBWbwcbzlk6p8+4O7vHZnh5ZJxyrU4+kyafTVEIr7l0iulKjdGpCqNTFQ5MhtepCvsnysn/PCZmeHlkgk2v7mf/ZJn6cf6zkU1b+O40hWyKjmyazkKG7nANpLsj29zOpVNk0kYmZWTSyZTaXDpFRy5NKZehlE/Tmc9QCj8d2TTZtOkxHTJnCxr6ZpYG/htwBbAD+LWZPeDuzy9kPSRuZsZgT4HBnsK8fF+97oxNV9g3UWb/ZIUDk8mJ4cBkMnV1ulpjulJjulJnplJjqlJjfKbKgakKr++bZGy6wthU9bA7rU9GyqCQTSc/mRSFXJpCJjnBFLJpOsK+fDaVnOAyKXKZMKMqPWs7k561nbw2vzecFAvZ5Ji6e/JzxLWT5N/38NeUWTiRJSe0bCo5melk1RoL3dO/DNjm7i8DmNndwLWAQl/OWKmUzWko6kSmKzUqtTq1ulOpeXhN1m6eLNeYmKkyUa4yPhO2Z6rNk8l0OJlMV+pMV2vMhO2p8D+WxnHlWnLiadw/0ep5HNm0kQ0nnmw61fyfztFOBQ7NE427N9+7E4b0khO6WWNYL3zOG58/1Nhm2VHaP/uk1ahJswyaJ6pmHe3Q9lv2zar7od/th5f54cc09n/4guXc/ImLj/IvcWoWOvRXAq/Per8DeO/sA8zsRuBGgDVr1ixczURarNGrXijuyU15jZvnmj+1cGG7mpxMZo44qZSrNVKpxnWTJGwb11GcQ4Gc/JIkmKt1p1qrU22e0OqUa0lZuZqc2Mq1OuWqU60f+3886XCNJgn25PcmbQknBD/8hHDcMG4G+aFSn5XAR4awM/sEcmjfW8P76GfSw05ldtjLW+pnBmf3FY/6Padq0V3IdffbgNsgmbLZ4uqItC0za/a0ybe6NrJQFnpawTCwetb7VaFMREQWwEKH/q+BdWa21sxywPXAAwtcBxGRaC3o8I67V83sXwI/I5myeae7P7eQdRARidmCj+m7+4PAgwv9e0VEZOGHd0REpIUU+iIiEVHoi4hERKEvIhKRRf08fTMbAba3uh6n2TJgb6srsYDU3vYVU1thcbf3bHfvP9qORR36MTCzTcda7KAdqb3tK6a2wpnbXg3viIhERKEvIhIRhX7r3dbqCiwwtbd9xdRWOEPbqzF9EZGIqKcvIhIRhb6ISEQU+gvIzO40sz1m9uyssj4ze8jMtobXJa2s43wxs9Vm9qiZPW9mz5nZF0N5u7a3YGa/MrPfhPb+h1C+1sweN7NtZvbD8EjxtmFmaTN7ysx+Et63bXvN7FUze8bMnjazTaHsjPt7VugvrO8AVx9R9mXgYXdfBzwc3reDKvAn7n4RsAG4ycwuon3bOwN8xN0vAd4NXG1mG4CvAbe6+7nAfuCGFtbxdPgisHnW+3Zv74fd/d2z5uefcX/PCv0F5O7/G9h3RPG1wF1h+y7gugWt1Gni7jvd/cmwfZAkGFbSvu11dx8Pb7Phx4GPAPeG8rZpL4CZrQI+Dtwe3htt3N5jOOP+nhX6rTfg7jvD9i5goJWVOR3MbAh4D/A4bdzeMNTxNLAHeAh4CTjg7tVwyA6SE1+7+Abwp0BjJfOltHd7Hfi5mT1hZjeGsjPu73nRLYweM3d3M2urObRm1gncB/yxu48lncFEu7XX3WvAu82sF/gxcEGLq3TamNk1wB53f8LMPtTq+iyQD7j7sJktBx4ysy2zd54pf8/q6bfebjNbARBe97S4PvPGzLIkgf99d/9RKG7b9ja4+wHgUeB9QK+ZNTpXq4DhllVsfr0f+AMzexW4m2RY55u0b3tx9+HwuofkpH4ZZ+Dfs0K/9R4ANobtjcD9LazLvAnju3cAm93967N2tWt7+0MPHzPrAK4guY7xKPDJcFjbtNfdv+Luq9x9CLgeeMTdP0ObttfMSmbW1dgGrgSe5Qz8e9YduQvIzH4AfIjkkay7gZuBvwPuAdaQPEb60+5+5MXeM46ZfQD4P8AzHBrz/TOScf12bO+7SC7kpUk6U/e4+1fN7BySnnAf8BTwWXefaV1N518Y3vk37n5Nu7Y3tOvH4W0G+Ft3v8XMlnKG/T0r9EVEIqLhHRGRiCj0RUQiotAXEYmIQl9EJCIKfRGRiCj0RUQiotAXEYnI/wc1GiF4Vo9LZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(y_firsts).map(lambda v: v.item() if v else None).value_counts().sort_index().plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9744090ad0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXxddZ3/8dcne5o9bZYm6ZLSjZYCXShFELAIFEYEFUfQGetMteNPHBfGhwOzoTM/R3w4isvP4TcVkKr8AEUGqhaRHRxsS1rKUrqle5a2abM0+/r9/XFPaqzplruce+95Px+PPHLvOSf3fg6nvHPyud/zPeacQ0REgiHF7wJERCR2FPoiIgGi0BcRCRCFvohIgCj0RUQCJM3vAk5lwoQJburUqX6XISKSUDZu3HjEOVcy2rq4Dv2pU6dSU1PjdxkiIgnFzPadbJ3aOyIiAaLQFxEJEIW+iEiAKPRFRAJEoS8iEiAKfRGRAFHoi4gEiEIfeHXXEV7f3+J3GSIiUafQB/7pibf5y/s3UHu4w+9SRESiKvCh75yjvqWbjt4BPv3TjXT0DvhdkohI1AQ+9I929tE7MMSyueXsburgy4+9Qe3hDh743R6++Ohm9h3t9LtEEZGIieu5d2KhobUbgA8uqGT+5EK+/tQ21r51EIAUg31HO/n5p99Faor5WaaISEQo9L3QryjM5uo5ZaSmGNkZqVw+o4Safc188dE3eOB3e/jU5dN8rlREJHyBD/361h4AKguzMTM++e4/hHtVUTZr3zrIN3+7nffMLmV6aa5fZYqIRETge/r1Ld1kp6dSOC79T9aZGV/7wHmMy0jlSz9/g8Eh50OFIiKRE/jQb2jtprIodJY/mtK8LL5yw1w2H2jlFxvrYlydiEhknTb0zewBMztsZm+PWPZNM9tmZm+a2X+bWeGIdXeaWa2ZbTeza0csX+YtqzWzOyK/K2PT0NZNRWH2Kbe58cIKLpxUyLee2U5332CMKhMRibwzOdN/EFh2wrJngPOcc+cDO4A7AcxsDnALMNf7mf80s1QzSwV+AFwHzAFu9bb1XUNrN5WFWafcxsz4h+vP5dCxXh74nz0xqkxEJPJOG/rOuZeB5hOW/dY5N3wV0zqgynt8I/CIc67XObcHqAUWe1+1zrndzrk+4BFvW1/19A9ypKOPioJTn+kDLK4u5uo5Zdz74i6OdvTGoDoRkciLRE//r4GnvMeVwIER6+q8ZSdb/ifMbKWZ1ZhZTVNTUwTKO7nh4ZqVRacPfYC/Xzab7v5Bvv98bTTLEhGJmrBC38z+ERgAHopMOeCcW+WcW+ScW1RSMurN3COmwRuuebqe/rDppbl8eGEVD63fx7Ge/miWJiISFWMOfTP7BPA+4GPOueGxjPXApBGbVXnLTrbcV8fP9M8w9AFuXlhF/6DjhW2Ho1WWiEjUjCn0zWwZ8GXg/c65rhGr1gC3mFmmmVUDM4ANwGvADDOrNrMMQh/2rgmv9PDVtXZjBmX5p/4gd6QFk4uYkJvJ01sORrEyEZHoOO0VuWb2MHAlMMHM6oC7CI3WyQSe8ca3r3POfdo5t8XMfga8Q6jtc5tzbtB7nc8CTwOpwAPOuS1R2J+z0tDaTWleJhlpZ/67LyXFuGZuGU+8Xk9P/yBZ6alRrFBEJLJOG/rOuVtHWXz/Kbb/GvC1UZavBdaeVXVRFhqueeatnWHXzi3n/63fz+92HuG9c8qiUJmISHQE+orchtbTX5g1mkumjScvK00tHhFJOIEN/aEhR0Nrz5jO9DPSUlg6u5Rntx5iYHCIvoEh/uulXWzSLRdFJM4FNvSPdPbSNzg0pjN9CLV4Wrr6+eWbDXxk1e/5+lPb+K+XdkW4ShGRyArs1MoNI6ZUHosrZpaQmZbCFx99g5yMVKaV5LD9YHskSxQRibjAnumPvHnKWORkpnHThZXMrcjnyc9exg3nV7CvuUsTsolIXAvsmX59y9lfmHWiuz807/iUzLPL83AOdh5u5/yqwtP8pIiIPwJ7pl/f2k1ORir52WP/vTdyDv6Z5XkAavGISFwLbOgfbu+hrCDrpDdPOVtTx+eQkZai0BeRuBbY0G/u7GN8TkbEXi81xZhRmsv2Qwp9EYlfgQ79onGRC32AWeV5OtMXkbgW6NAfnxvh0C/L43B7Ly2dfRF9XRGRSAlk6A8NOVq6+imOYHsHQmf6gFo8IhK3Ahn6x3r6GRxyEW/vzC7PB2CHQl9E4lQgQ/+o136JdHunLD+T/Kw0tqmvLyJxKpChP9xzL87JjOjrmhmzy/PZodAXkTgVyNAfPtMvjnB7B2BmeWjY5h/uICkiEj8CGfrNw6Ef4fYOwKzyfNp7Bmhs64n4a4uIhCvYoR+FM/1ZZZqOQUTiV2BDPzs9leyMyN/fdlZ5Hmaw+UBrxF9bRCRcgQz9ls6+iI/RH1aQnc75VYW8vLMpKq8vIhKOQIb+0ShcjTvSFTMm8MaBVtq6+qP2HiIiYxHI0I/GvDsjXT6zhCEHv6s9ErX3EBEZi9OGvpk9YGaHzeztEcuKzewZM9vpfS/ylpuZfc/Mas3sTTNbMOJnlnvb7zSz5dHZnTMT6Rk2T3ThpELystJ4eYdaPCISX87kTP9BYNkJy+4AnnPOzQCe854DXAfM8L5WAvdC6JcEcBdwMbAYuGv4F4UfmqPY0wdIS03hsukTeHlnk8bri0hcOW3oO+deBppPWHwjsNp7vBq4acTyH7uQdUChmU0ErgWecc41O+dagGf4018kMdHdN0h3/yBFUQx9CLV4Gtt62Hm4I6rvIyJyNsba0y9zzjV6jw8CZd7jSuDAiO3qvGUnW/4nzGylmdWYWU1TU+TbI81d3rw7MQh9QC0eEYkrYX+Q60L9i4j1MJxzq5xzi5xzi0pKSiL1ssc1dwzPuxPd0K8szGZ6aS4vKfRFJI6MNfQPeW0bvO+HveX1wKQR21V5y062POaOdvYCkZ9hczSXzyhhw55mevoHo/5eIiJnYqyhvwYYHoGzHHhyxPKPe6N4lgBtXhvoaeAaMyvyPsC9xlsWcy1eeyeaQzaHXTmrhN6BIV7cfvj0G4uIxMCZDNl8GPg9MMvM6sxsBXA3cLWZ7QTe6z0HWAvsBmqBHwKfAXDONQP/Brzmff2rtyzmjnYM9/QjO63yaN51znjK87N4eMOB028sIhIDaafbwDl360lWXTXKtg647SSv8wDwwFlVFwXNnX2kphj52afd9bClpabw5xdN4vvP7+RAcxeTisdF/T1FRE4lcFfktnSFrsY1s5i83y0XTcKAR17bH5P3ExE5lcCF/tGO6F6Ne6KKwmzeM6uUn9XU0T84FLP3FREZTeBCP9pX447moxdPpqm9l2ffORTT9xUROVH0G9txprmrj3PL82P6nlfOKqWiIItVr+zmcHsvjW09zCjN5UMLq2Jah4hI8ELfhzP91BTjY0um8M2nt/P6/tDNVbLTU3n/hRWkpwbujy0R8VGgQn9gcIi27v6Yhz7A31w+jaWzS5mQm8mru47w+Uc2s62xnXlVBTGvRUSCK1Cnma3d/TgX/SkYRpOWmsK5E/MpycvkoqnFANTs8+VSBREJsECF/vEbovsQ+iNVFGYzsSCLmn0tvtYhIsETyNCP5ZDNk1k4pYhNCn0RibFAhn6059I/EwunFNHY1kN9a7ffpYhIgAQq9I/G0Zn+oimhvv5Gne2LSAwFKvSbjvWQYvFxpn/uxDyy01PZuFcf5opI7AQq9OtbeyjLz4qLsfFpqSlcOKmQjft1pi8iseN/+sVQY1s3Ewuy/C7juEVTi9ja2E5n74DfpYhIQAQq9Btau6kozPa7jOMWTilicMix+UCr36WISEAEJvSdczS09cRV6M+fXISZPswVkdgJTOgf7eyjb2CIijhq7xRkpzO7PJ+ntxwkdP8ZEZHoCkzoN7b2ADAxjs70Af760qlsaTjG01s07bKIRF9gQn/4IqjKOAv9D8yvZNqEHL79zHYGh3S2LyLRFZjQb2wLhX48jd6B0NDNL1w9kx2HOvjVmw1+lyMiSS4wod/Q2k1mWorvk62N5n3zJjK7PI/vPLuTAd1SUUSiKDih743cidUN0c9GSopx+9Uz2XOkk3ue3cGQ2jwiEiVhhb6ZfdHMtpjZ22b2sJllmVm1ma03s1oze9TMMrxtM73ntd76qZHYgTMVGqMfX62dka6eU8ZNF1bwgxd28Rf3r+dgW4/fJYlIEhpz6JtZJfA5YJFz7jwgFbgF+AZwj3NuOtACrPB+ZAXQ4i2/x9suZhpbe5hYEF8f4o5kZtzzkQv5xofm8fr+VpZ992X2HOn0uywRSTLhtnfSgGwzSwPGAY3AUuAxb/1q4Cbv8Y3ec7z1V1mMei39g0Mcau+JqzH6ozEzPnLRZNZ89lJau/pZ+1aj3yWJSJIZc+g75+qB/wD2Ewr7NmAj0OqcG55Mpg6o9B5XAge8nx3wth9/4uua2UozqzGzmqamprGW90cOHevBOeLqatxTmVGWx6yyPNbv0QycIhJZ4bR3igidvVcDFUAOsCzcgpxzq5xzi5xzi0pKSsJ9OQAa2+LzwqxTWVxdzMa9zRrNIyIRFU57573AHudck3OuH3gcuBQo9No9AFVAvfe4HpgE4K0vAI6G8f5nrOH4hVnx3d4ZaXF1MZ19g2xtbPe7FBFJIuGE/n5giZmN83rzVwHvAC8AN3vbLAee9B6v8Z7jrX/exWjCmYbhKRji+IPcEy2uDt1Za/2emPxeFJGACKenv57QB7KbgLe811oF/D1wu5nVEurZ3+/9yP3AeG/57cAdYdR9VhpauynITicnM+30G8eJsvwspowfxwb19UUkgsJKQefcXcBdJyzeDSweZdse4MPhvN9YxdvNU87U4qnFPLv1EM65uLyoTEQSTyCuyK1v7Ym7idbOxEXVxbR09VN7uMPvUkQkSQQi9BvbupmYQB/iDrv4eF9fLR4RiYykD/2uvgFau/oTZoz+SJOLx1GWn6m+vohETOJ8sjlGwyN3KhJo5M4wM2Nx9Xg27Glmx6F22nsGGJ+TwdQJOX6XJiIJKulD/9CxUOiXJ+AHuRBq8fzyjQauuedlAHIz09j0z1eTkZb0f6SJSBQkfei39/QDkJ+V7nMlY3PzwipyMlNJT01h56EOvvvcTt5pPMaFkwr9Lk1EElDSny6294SmAcrLSszfb1npqXxgfhXvO7+CWxdPBmDTvhafqxKRRJX0od/ZGwr9RLow62TKC7KoLMxm436FvoiMTdKHfsfx0E/1uZLIWDClSGf6IjJmAQj9QTJSU8hMS47QXzi5kMa2nuOTyImInI0AhH4/uQnazx/NwimhC7Y26mxfRMYg6UO/s3cwaVo7ALMn5pGdnsom9fVFZAySPvTbewbIzUzM4ZqjSU9N4YJJBerri8iYJH3od/YOkJtEZ/oACyYXsaXhGN19g36XIiIJJulDv6N3gNwkGK450sIpRQwMOd6sa/W7FBFJMIEI/WQYoz/S/MlFABqvLyJnLbnScBQdvQMJezXuyRTnZDCtJIef/H4ftYc6KC/I4oMLqphemut3aSIS55L/TL9ngJyM5Ap9gM9cOZ3KwmzW72nm/760i7uf2uZ3SSKSAJIvDUcYHHJ09w8m1Tj9YTcvrOLmhVUA/NMTb/H4pnr6BoY0+6aInFJSJ8TwFAzJ9kHuia6YWUpX3yA1e3WzFRE5taQO/c6AhP67zhlPeqrx0o4mv0sRkTiX1KHfkUQzbJ5KTmYaF00tVuiLyGmFFfpmVmhmj5nZNjPbamaXmFmxmT1jZju970XetmZm3zOzWjN708wWRGYXTu54eycJe/onumJmCdsOttPYponYROTkwj3T/y7wG+fcbOACYCtwB/Ccc24G8Jz3HOA6YIb3tRK4N8z3Pq2O4RuoJPmZPsCVs0oBeFln+yJyCmMOfTMrAC4H7gdwzvU551qBG4HV3margZu8xzcCP3Yh64BCM5s45srPQDLdQOV0ZpblUp6fpRaPiJxSOGf61UAT8CMze93M7jOzHKDMOdfobXMQKPMeVwIHRvx8nbcsatoD8kEugJlxxcwSXtl5hIHBIb/LEZE4FU7opwELgHudc/OBTv7QygHAOecAdzYvamYrzazGzGqamsI7aw3K6J1hV8wqob1ngNW/30dDazeh//wiIn8QTujXAXXOufXe88cI/RI4NNy28b4f9tbXA5NG/HyVt+yPOOdWOecWOecWlZSUhFHeH3r6QWjvAFw2YwJVRdn826/e4V13P8/Sb72kO2yJyB8Zc+g75w4CB8xslrfoKuAdYA2w3Fu2HHjSe7wG+Lg3imcJ0DaiDRQVHb0DZKSlBOYq1fysdF740pWs+eyl/PP75rDnSCf//fqf/F4VkQAL9xT4b4GHzCwD2A38FaFfJD8zsxXAPuDPvW3XAtcDtUCXt21UdfQOBGLkzkjpqSmcX1XI+VWFrNlcz2+3HOS290z3uywRiRNhJaJzbjOwaJRVV42yrQNuC+f9zlYyTqt8Nq6ZW843n95OQ2s3FYXZfpcjInEgqfsenUl4A5Wzsey8cgB+u+Wgz5WISLxI6tAP3R83uKF/Tkku00tzeXrLIb9LEZE4kdSh39k3EIgpGE7l2rllbNjbTEtnn9+liEgcSOrQ7+gJdk8f4Nq55QwOOZ7dqrN9EUn20O8dDHR7B2BeZQEVBVlq8YgIkOR3zuro7Sc3M9XvMnxlZlwzt5wHX93LBV/9LXlZaVw2fQJf/+A8zMzv8kQkxpI29AcGh+jpHyI3M93vUnz36SvOIS8rjWPd/dS1dPPIawe4YFIhty6e7HdpIhJjSRv6nb2DAOQE/EwfoLwgi7+7JnTh9NCQ46P3rePf125l6exSyvKzfK5ORGIpaXv6HX3eXPoBH71zopQU4+4Pnk/fwBD//MTbmpRNJGCSN/R7hmfYVHvnRFMn5HD71TP57TuHeOptXbglEiTJG/rHb6Ci9s5oVlxWzbzKAv7lyS20dmkMv0hQJH3oq70zurTUFO7+0Dxauvr437/e6nc5IhIjyRv6AZtLfyzmVhTwN5dP47GNdbyyU7dZFAmCpA39oN01a6w+d9UMpk3I4c7H3zr+30xEklfShn6Q7o8bjqz0VL7+wXnUtXTzXy/t8rscEYmypA39zl61d87UxdPGc82cMn68bh9dfTrbF0lmSRv6Hb0DZKalkJ6atLsYUSsvn0ZrVz+PbazzuxQRiaKkTcSO3gGN3DkLC6cUMX9yIfe9sofBIV2wJZKskjf0Na3yWTEzVr57Gvubu3SnLZEklrShH/RbJY7FNXPLmVw8jh++stvvUkQkSpI29NsDflP0sUhNMVZcVs2m/a1s2NPsdzkiEgVJG/qdvQPkKfTP2ocXVVGSl8k3frNNk7GJJKGkDf0OnemPybiMNG6/eiYb97XoblsiSSjs0DezVDN73cx+5T2vNrP1ZlZrZo+aWYa3PNN7Xuutnxrue59KZ69uij5WH15YxfTSXL7xm230Dw75XY6IRFAkzvQ/D4ycsesbwD3OuelAC7DCW74CaPGW3+NtFzXtPfogd6zSUlO487rZ7DnSySMb9vtdjohEUFihb2ZVwJ8B93nPDVgKPOZtshq4yXt8o/ccb/1VFqWbtPYPDtE7MKTQD8PS2aVcXF3Md57dyfaD7X6XIyIREu6Z/neALwPDPYDxQKtzbvha/jqg0ntcCRwA8Na3edv/ETNbaWY1ZlbT1DS2mR81BUP4zIy7bpiLA274/u/4wQu1DAwO0dzZx/rdRznQ3OV3iSIyBmNORTN7H3DYObfRzK6MVEHOuVXAKoBFixaNafhISorxqXdXc0FVQaTKCqQ5Ffk888XL+Zc1W/jm09v5P8/X0t0fuvfwOSU5PHv7FUTpjzURiZJwToUvBd5vZtcDWUA+8F2g0MzSvLP5KqDe274emATUmVkaUAAcDeP9Tyo/K51//LM50XjpwBmfm8kPPrqAG84/yCs7m6iekENDaw8P/M8edhzqYFZ5nt8lishZGHN7xzl3p3Ouyjk3FbgFeN459zHgBeBmb7PlwJPe4zXec7z1zzsNBE8Yy84r52sfmMcn3z2N/3XlOZjB2rca/S5LRM5SNMbp/z1wu5nVEurZ3+8tvx8Y7y2/HbgjCu8tMVCSl8niqcU89bZCXyTRROSTTufci8CL3uPdwOJRtukBPhyJ9xP/XT9vInet2ULt4Xaml6rFI5IokvaKXImuZeeVA7D2Lc3IKZJIFPoyJmX5WSyaUqS+vkiCUejLmF03byLbDrazu6nD71JE5Awp9GXMhls8T72tFo9IolDoy5hVFmZzQVUBz27VbJwiiUKhL2FZOruMzQdaOdLR63cpInIGFPoSlqvOLcU5eHH72OZJEpHYUuhLWOZW5FOWn8nz29TiEUkECn0Ji5mxdHYpL+84Qt+AbrgiEu8U+hK2pbPL6Ogd4LW9upm6SLxT6EvYLp0+noy0FJ7fdtjvUkTkNBT6ErZxGWlcMm28Ql8kASj0JSKuOreUPUc6dXWuSJxT6EtELJ1dCsCTmxt8rkRETkWhLxFRVTSO955byurf7z1+j2IRiT8KfYmYz7xnOq1d/Ty8Yb/fpYjISSj0JWIWTC7ikmnj+eEru+kdGPS7HBEZhUJfIuoz7zmHQ8d6eXxTvd+liMgoFPoSUZdNn8D5VQXc++IuBgZ1ha5IvFHoS0SZGZ99z3T2N3fx6Z9uokMf6orEFYW+RNzVc8r4yg1zeH7bIW6+91UONHf5XZKIeBT6EnFmxicurebBv1pMfWs3H7z3Vdp7+v0uS0QII/TNbJKZvWBm75jZFjP7vLe82MyeMbOd3vcib7mZ2ffMrNbM3jSzBZHaCYlPl88s4UefuIim9l6eeF0f7IrEg3DO9AeAv3POzQGWALeZ2RzgDuA559wM4DnvOcB1wAzvayVwbxjvLQli4ZQizqvM56fr9uOc87sckcAbc+g75xqdc5u8x+3AVqASuBFY7W22GrjJe3wj8GMXsg4oNLOJY65cEoKZ8RcXT2H7oXZq9rX4XY5I4EWkp29mU4H5wHqgzDnX6K06CJR5jyuBAyN+rM5bduJrrTSzGjOraWrSLfiSwfsvrCAvK42frtvndykigRd26JtZLvAL4AvOuWMj17nQ3/Nn9Te9c26Vc26Rc25RSUlJuOVJHBiXkcaHFlTx1FsHOaobqIv4KqzQN7N0QoH/kHPucW/xoeG2jfd9eJL1emDSiB+v8pZJAPzFksn0DQ7xs5o6v0sRCbRwRu8YcD+w1Tn37RGr1gDLvcfLgSdHLP+4N4pnCdA2og0kSW56aR5LphXz4Kt7aOvS8E0Rv4Rzpn8p8JfAUjPb7H1dD9wNXG1mO4H3es8B1gK7gVrgh8BnwnhvSUD/cP25NHf2ccfjb2okj4hP0sb6g8653wF2ktVXjbK9A24b6/tJ4ju/qpAvXTOLrz+1jUdeO8Ctiyf7XZJI4OiKXImpT717Gu+eMYGv/nILtYfb/S5HJHAU+hJTKSnGtz58AeMy0vjrB2toaO32uySRQFHoS8yV5mdx//JFtHT2ccuqddQr+EViRqEvvpg/uYiffPJiWrr6uGXV73XGLxIjCn3xzYWTCvnpios52tHH3U9t87sckUBQ6IuvLphUyC0XTWbtW40cOtbjdzkiSU+hL777+CVTGHSOhzQ3j0jUKfTFd1Mn5LB0VikPrd9P78Cg3+WIJDWFvsSFT1w6laOdffzqDc3MIRJNCn2JC5dNn8D00lwefHWvpmgQiSKFvsQFM2P5u6byVn0bz249fPofEJExUehL3Lh5QRXnTsznbx/exPrdR/0uRyQpKfQlbmRnpPKTFYupLMxmxeoa3jjQ6ndJIklHoS9xZUJuJg99cglFOeks/9EGdjd1+F2SSFJR6EvcKS/I4qEVS0gx45M/ruFYj266IhIpCn2JS5PHj+M/P7aA/Ue7+NzDrzM4pBE9IpGg0Je4tWTaeP71xvN4cXsTt/9sMz9dt48nN9dT19Lld2kiCWvMd84SiYWPXjyZPUc6+OEre3hycwMAxTkZ/PpzlzGxINvn6kQSj8XzhTCLFi1yNTU1fpchcaCrb4COngH2Hu3iEz/awNyKfB7+1BLSUvXHqsiJzGyjc27RaOv0f4wkhHEZaZTmZ7G4uph//8A8Xtvbwree2eF3WSIJR6EvCeem+ZXcungS9764iyder9e0DSJnQT19SUh33TCXrY3tfOHRzfzyjQbuumEuB4/18Mhr+1m36yiXTp/ALYsnsWByEWbmd7kicSPmPX0zWwZ8F0gF7nPO3X2ybdXTl1PpHxziwf/Zyz3P7qCrLzQlc25mGhdXF7Nu91E6+waZVJzN3IkFzCzLZU5FAe+aPp78rHSfKxeJrlP19GMa+maWCuwArgbqgNeAW51z74y2vUJfzkRjWzerX93H9NJcrp9XzriMNDp7B/j1m408t+0QOw91sPdoJ0MOUlOM+ZMKqSjMpqN3gI7eAfKz0phYkE1Zfibp3gfDKWbkZKaRm5VGdnoqw38rjMtMZWJBNuX5WaSkQHvPAF29gxTmpJOXmaa/KiQuxFPoXwJ8xTl3rff8TgDn3NdH216hL5HS0z/Im3VtvLyjiVd2NtHW3U9uVhrjMtI41t1PY1sPbd3hXfmbk5FKWX4WqSkKfgnf7In5fP/W+WP62VOFfqx7+pXAgRHP64CLR25gZiuBlQCTJ0+OXWWS1LLSU1lcXczi6mK+dO2sUbfp6R9kyDsJGhxydPYO0tHbT3ff0PFt2nv7aWzt4aB3P9887y+B1q7QL47D7T3HX0MkHJOKonMdStx9kOucWwWsgtCZvs/lSIBkpaf+0fO8rHQgy59iRKIk1kM264FJI55XectERCQGYh36rwEzzKzazDKAW4A1Ma5BRCSwYtrecc4NmNlngacJDdl8wDm3JZY1iIgEWcx7+s65tcDaWL+viIhoGgYRkUBR6IuIBIhCX0QkQBT6IiIBEtc3UTGzJmDfiEUTgCM+lRMryb6P2r/El+z7mAz7N8U5VzLairgO/ROZWc3J5pNIFsm+j9q/xJfs+5js+6f2johIgCj0RUQCJNFCf5XfBcRAsu+j9i/xJfs+JvX+JVRPX0fg9OgAAAOBSURBVEREwpNoZ/oiIhIGhb6ISIAkTOib2TIz225mtWZ2h9/1hMvMJpnZC2b2jpltMbPPe8uLzewZM9vpfS/yu9ZwmFmqmb1uZr/ynleb2XrvOD7qTbGdsMys0MweM7NtZrbVzC5JpmNoZl/0/n2+bWYPm1lWoh9DM3vAzA6b2dsjlo16zCzke96+vmlmC/yrPDISIvS9G6r/ALgOmAPcamZz/K0qbAPA3znn5gBLgNu8fboDeM45NwN4znueyD4PbB3x/BvAPc656UALsMKXqiLnu8BvnHOzgQsI7WtSHEMzqwQ+Byxyzp1HaDr0W0j8Y/ggsOyEZSc7ZtcBM7yvlcC9MaoxahIi9IHFQK1zbrdzrg94BLjR55rC4pxrdM5t8h63EwqLSkL7tdrbbDVwkz8Vhs/MqoA/A+7znhuwFHjM2yTR968AuBy4H8A51+ecayWJjiGh6dezzSwNGAc0kuDH0Dn3MtB8wuKTHbMbgR+7kHVAoZlNjE2l0ZEooT/aDdUrfaol4sxsKjAfWA+UOecavVUHgTKfyoqE7wBfBobvLD4eaHXODXjPE/04VgNNwI+8FtZ9ZpZDkhxD51w98B/AfkJh3wZsJLmO4bCTHbOky55ECf2kZWa5wC+ALzjnjo1c50LjaRNyTK2ZvQ847Jzb6HctUZQGLADudc7NBzo5oZWT4MewiNCZbjVQAeTwp22RpJPIx+xMJEroJ+UN1c0snVDgP+Sce9xbfGj4z0fv+2G/6gvTpcD7zWwvoXbcUkL970KvVQCJfxzrgDrn3Hrv+WOEfgkkyzF8L7DHOdfknOsHHid0XJPpGA472TFLuuxJlNBPuhuqe/3t+4Gtzrlvj1i1BljuPV4OPBnr2iLBOXenc67KOTeV0PF63jn3MeAF4GZvs4TdPwDn3EHggJnN8hZdBbxDkhxDQm2dJWY2zvv3Orx/SXMMRzjZMVsDfNwbxbMEaBvRBkpMzrmE+AKuB3YAu4B/9LueCOzPZYT+hHwT2Ox9XU+o7/0csBN4Fij2u9YI7OuVwK+8x9OADUAt8HMg0+/6wty3C4Ea7zg+ARQl0zEEvgpsA94GfgJkJvoxBB4m9BlFP6G/1lac7JgBRmjk4C7gLUIjmXzfh3C+NA2DiEiAJEp7R0REIkChLyISIAp9EZEAUeiLiASIQl9EJEAU+iIiAaLQFxEJkP8PXlg4NhcrQMwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(y_lasts).map(lambda v: v.item() if v else None).value_counts().sort_index().plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>af3fed7fc3</td>\n",
       "      <td>is back home now      gonna miss every one</td>\n",
       "      <td>onna</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>fdb77c3752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5436</th>\n",
       "      <td>780c673bca</td>\n",
       "      <td>going out for the good ol` `soak` tonight for ...</td>\n",
       "      <td>ing</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8729</th>\n",
       "      <td>12f21c8f19</td>\n",
       "      <td>star wars ............ is **** BOO??? i wanna...</td>\n",
       "      <td>l</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11808</th>\n",
       "      <td>96ff964db0</td>\n",
       "      <td>4 hours of sleep, a migraine, again? What is w...</td>\n",
       "      <td>hat</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12405</th>\n",
       "      <td>df398a774e</td>\n",
       "      <td>alright...I`m going to get off comp now, go ...</td>\n",
       "      <td>ave</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14172</th>\n",
       "      <td>c155efab1b</td>\n",
       "      <td>Hey Honey  Bunny here  big bunny hugs</td>\n",
       "      <td>unny</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15207</th>\n",
       "      <td>4b3fd4ec22</td>\n",
       "      <td>Last day at DMA over!     a million sad faces.</td>\n",
       "      <td>ion</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21983</th>\n",
       "      <td>4c279acff6</td>\n",
       "      <td>why? i enjoy fancy meals on my own smtimes, t...</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24929</th>\n",
       "      <td>09d0f8f088</td>\n",
       "      <td>two macaroons go into a bar....one says oh yo...</td>\n",
       "      <td>wo</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "18     af3fed7fc3         is back home now      gonna miss every one   \n",
       "314    fdb77c3752                                                NaN   \n",
       "5436   780c673bca  going out for the good ol` `soak` tonight for ...   \n",
       "8729   12f21c8f19   star wars ............ is **** BOO??? i wanna...   \n",
       "11808  96ff964db0  4 hours of sleep, a migraine, again? What is w...   \n",
       "12405  df398a774e    alright...I`m going to get off comp now, go ...   \n",
       "14172  c155efab1b              Hey Honey  Bunny here  big bunny hugs   \n",
       "15207  4b3fd4ec22     Last day at DMA over!     a million sad faces.   \n",
       "21983  4c279acff6   why? i enjoy fancy meals on my own smtimes, t...   \n",
       "24929  09d0f8f088   two macaroons go into a bar....one says oh yo...   \n",
       "\n",
       "      selected_text sentiment  \n",
       "18             onna  negative  \n",
       "314             NaN   neutral  \n",
       "5436            ing  positive  \n",
       "8729              l  positive  \n",
       "11808           hat  negative  \n",
       "12405           ave  positive  \n",
       "14172          unny  positive  \n",
       "15207           ion  negative  \n",
       "21983           joy  positive  \n",
       "24929            wo  positive  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall the two rubbish cases: NaN values and selected texts not corresponding with token starts\n",
    "train.iloc[bad_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_orig = train\n",
    "X_train_df = train.iloc[good_idxs].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prototype model part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterSentimentExtractionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # configuring the model to output hidden states\n",
    "        cfg = transformers.PretrainedConfig.get_config_dict(\"bert-base-uncased\")[0]  # tuple?\n",
    "        cfg[\"output_hidden_states\"] = True\n",
    "        cfg = transformers.BertConfig.from_dict(cfg)\n",
    "        self.bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\", config=cfg)\n",
    "        \n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.l0 = nn.Linear(768 * 2, 2)\n",
    "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
    "        # self.out = nn.LogSoftmax(dim=-2)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # ignore the output from the model head, we'll instead we'll construct our own attention\n",
    "        # head connected to the last two layers of hidden weights.\n",
    "        # that's 512x762x2=780288 connections.\n",
    "        _, _, out = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        out = torch.cat((out[-1], out[-2]), dim=-1)\n",
    "        out = self.drop_out(out)\n",
    "        # the new head uses a linear layer with two output nodes.\n",
    "        # the first node learns sequence start.\n",
    "        # the second node learns sequence end.\n",
    "        logits = self.l0(out)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        \n",
    "        return start_logits, end_logits\n",
    "        \n",
    "        # TODO: embed softmax directly into the model arch\n",
    "        # y_start, y_end = self.out(start_logits), self.out(end_logits)\n",
    "        # return y_start, y_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it\n",
    "\n",
    "dataset = TwitterSentimentExtractionDataset(X_train_df)\n",
    "model = TwitterSentimentExtractionModel()\n",
    "ex = dataset[0]\n",
    "ex_input_ids, ex_attention_mask, ex_token_type_ids =\\\n",
    "    ex['input_ids'][None,:], ex['attention_mask'][None,:], ex['token_type_ids'][None,:]\n",
    "\n",
    "result = model(ex_input_ids, ex_attention_mask, ex_token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    y_pred_first_loss = ce(start_logits, start_positions)\n",
    "    y_pred_last_loss = ce(end_logits, end_positions)\n",
    "    y_pred_loss = (y_pred_first_loss + y_pred_last_loss)\n",
    "    return y_pred_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.6803, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try it\n",
    "\n",
    "# loss_fn(result[0], result[1], torch.tensor([1]), torch.tensor([5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 8699,  102,  ...,    0,    0,    0],\n",
       "         [ 101, 4997,  102,  ...,    0,    0,    0],\n",
       "         [ 101, 4997,  102,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 4997,  102,  ...,    0,    0,    0],\n",
       "         [ 101, 3893,  102,  ...,    0,    0,    0],\n",
       "         [ 101, 4997,  102,  ...,    0,    0,    0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'y_first': tensor([ 4,  4,  7,  7,  4,  3, 12,  3,  4,  7,  4,  7,  9, 13,  3,  3, 15,  8,\n",
       "          3,  4,  9,  3,  3,  3,  3,  4, 11, 18, 16, 19,  5, 24,  8,  4,  3, 21,\n",
       "          3,  7,  3,  4,  6,  3,  3,  4,  3, 23,  3, 16, 15,  4,  4,  4,  4,  4,\n",
       "          4, 23,  3,  6,  4,  3,  4,  3, 27, 19]),\n",
       " 'y_last': tensor([13,  6,  8,  9, 10, 33, 12,  7,  6, 15, 30,  7,  9, 13, 13, 12, 17,  8,\n",
       "         11, 26, 10, 19, 26, 36, 18,  8, 13, 18, 21, 24,  5, 33,  9, 30, 37, 21,\n",
       "         29, 17, 29, 27,  8,  8, 41,  7, 26, 26, 11, 19, 24, 27, 20, 15, 24,  7,\n",
       "         19, 23, 26,  9, 10, 22,  6,  9, 27, 21])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try it\n",
    "\n",
    "X_train = TwitterSentimentExtractionDataset(X_train_df)\n",
    "X_train_dataloader = torch.utils.data.DataLoader(\n",
    "    X_train,\n",
    "    batch_size=64,\n",
    "    num_workers=1\n",
    ")\n",
    "iter(X_train_dataloader).next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prototype training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = TwitterSentimentExtractionModel()\n",
    "model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer; uses weight decay\n",
    "model_parameters = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "    {'params': [p for n, p in model_parameters if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "    {'params': [p for n, p in model_parameters if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "optimizer = AdamW(optimizer_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size and epochs globals; setting batch_size = 1 for testing, 64 is too much for the K80\n",
    "batch_size = 1\n",
    "# batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset and dataloader\n",
    "dataset = TwitterSentimentExtractionDataset(X_train_df)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scheduler, this one is a transformers module hookup, thanks huggingface\n",
    "num_train_steps = int(len(dataloader) * epochs)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_train_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train func for one epoch of training\n",
    "def train_fn(dataloader, model, optimizer, device, scheduler, epoch_num):\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    fn = lambda field: records[field].to(device, dtype=torch.long)\n",
    "    for idx, records in enumerate(dataloader):\n",
    "        # move the record to GPU\n",
    "        input_ids = fn(\"input_ids\")\n",
    "        token_type_ids = fn(\"token_type_ids\")\n",
    "        attention_mask = fn(\"attention_mask\")\n",
    "        y_first = fn(\"y_first\")\n",
    "        y_last = fn(\"y_last\")\n",
    "        \n",
    "        model.zero_grad()\n",
    "        y_pred_start_logits, y_pred_end_logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        loss = loss_fn(y_pred_start_logits, y_pred_end_logits, y_first, y_last)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        y_pred_starts = torch.softmax(y_pred_start_logits, dim=1).cpu().detach().numpy()\n",
    "        y_pred_ends = torch.softmax(y_pred_end_logits, dim=1).cpu().detach().numpy()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if idx % 10 == 0:\n",
    "            print(f\"epoch {epoch_num}, batch {idx} training loss: {losses[-1]}\")\n",
    "        \n",
    "        break\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 0 training loss: 9.667827606201172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9.667827606201172]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try it; note, first you should unhide the break\n",
    "\n",
    "train_fn(dataloader, model, optimizer, device, scheduler, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    checkpoints_dir = \"/spell/checkpoints/\"\n",
    "    if not os.path.exists(checkpoints_dir):\n",
    "        os.mkdir(checkpoints_dir)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_fn(dataloader, model, optimizer, device, scheduler, epoch)\n",
    "        torch.save(model.state_dict(), f\"/spell/checkpoints/model_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write model definition to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../models/model_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../models/model_1.py\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "# from sklearn import model_selection\n",
    "# from sklearn import metrics\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from tqdm.autonotebook import tqdm\n",
    "from collections import namedtuple\n",
    "\n",
    "train = pd.read_csv(\"/mnt/tweet-sentiment-extraction/train.csv\")\n",
    "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
    "    # yes this really happens lol, try idx 314\n",
    "    if pd.isnull(tweet) or pd.isnull(selected_text) or len(tweet) == 0 or len(selected_text) == 0:\n",
    "        raise ValueError(\"text or selected_text is nan.\")\n",
    "    \n",
    "    # get indicial boundaries of substring.\n",
    "    target_char_idx_start = tweet.index(selected_text)\n",
    "    target_char_idx_end = target_char_idx_start + len(selected_text)\n",
    "\n",
    "    # build the character attention mask (used to build the token attention mask)\n",
    "    char_target_mask = (\n",
    "        [0] * target_char_idx_start +\n",
    "        [1] * (target_char_idx_end - target_char_idx_start) +\n",
    "        [0] * (len(tweet) - target_char_idx_end)\n",
    "    )\n",
    "    \n",
    "    # tokenize\n",
    "    # `ids` is the token values, `offsets` are the position tuples for the tokes in the str\n",
    "    tokens_obj = tokenizer.encode(tweet)\n",
    "    token_ids, token_offsets = tokens_obj.ids, tokens_obj.offsets\n",
    "    \n",
    "    # this is the clever bit. recall that the task is to find the subsequence in the sequence\n",
    "    # exemplifying the given sentiment. to do this we reformulate the input sequence as a\n",
    "    # question-answer pair, where the sentiment (as a single word) is the question and the\n",
    "    # sequence as a whole is the answer.\n",
    "    # \n",
    "    # this allows us to use version of the BERT model pretrained on the well-formed and\n",
    "    # well-studied question-answering task as a surrogate for this task.\n",
    "    sentiment_id_map = {\n",
    "        'positive': 3893,\n",
    "        'negative': 4997,\n",
    "        'neutral': 8699\n",
    "    }\n",
    "    # 101 is [CLS] and 102 is [SEP]. BERT expects Q/A input to be in the form\n",
    "    # [CLS] [...] [SEP] [...] [SEP]. Cf.\n",
    "    # https://huggingface.co/transformers/glossary.html#token-type-ids\n",
    "    # NOTE: the [-1:1] is the excise the start-of-seq and end-of-seq in the tokens\n",
    "    input_ids = [101] + [sentiment_id_map[sentiment]] + [102] + token_ids[1:-1] + [102]\n",
    "    \n",
    "    # BERT expects Q/A pairs to come with a binary mask splitting the pair types\n",
    "    # NOTE: the mafs excludes start-of-seq and end-of-seq but includes the new end-of-seq\n",
    "    token_type_ids = [0, 0, 0] + [1] * (len(token_ids) - 2 + 1)\n",
    "\n",
    "    # pad to max_len and create a corresponding attention mask\n",
    "    pad_len = max_len - len(input_ids)\n",
    "    attention_mask = [1] * len(input_ids) + [0] * pad_len\n",
    "    input_ids = input_ids + [0] * pad_len\n",
    "    token_type_ids = token_type_ids + [0] * pad_len\n",
    "    \n",
    "    # get the index of the first and last token of the target, this is what the model will try\n",
    "    # to predict! see the notes on the head layer in forward for more info.\n",
    "    # we add 3 because the first thee elements of the mask are always [CLS] $SENTIMENT [CLS]\n",
    "    # and always get an attention vector [1 1 1].\n",
    "    ufunc = lambda first, _: first >= target_char_idx_start and first < target_char_idx_end\n",
    "    y_pred_mask = [ufunc(*offset) for offset in token_offsets]\n",
    "    try:\n",
    "        y_first = 3 + y_pred_mask.index(True)\n",
    "        y_last = 3 + len(y_pred_mask) - y_pred_mask[::-1].index(True) - 1\n",
    "    except ValueError:\n",
    "        # some of the labels are noisy, and the first character in the label does not actually\n",
    "        # correspond with the first character of any token (e.g. the label is a part-of-a-word\n",
    "        # instead of a word). I'm going to venture the opinion here that these records \n",
    "        # constitute data noise (because, I mean, they are) and should be removed in\n",
    "        # pre-processing\n",
    "        raise ValueError(\n",
    "            f\"Found bad selected_text value '{selected_text}' for tweet '{tweet}'.\"\n",
    "            f\"Make sure to get rid of these in a pre-processing pass.\"\n",
    "        )\n",
    "    \n",
    "    # convert to torch tensors\n",
    "    t = lambda seq: torch.tensor(seq, dtype=torch.long)\n",
    "    input_ids, token_type_ids, attention_mask, y_first, y_last =\\\n",
    "        t(input_ids), t(token_type_ids), t(attention_mask), t(y_first), t(y_last)\n",
    "    \n",
    "    # output\n",
    "    # Unfortunately the PyTorch dataloader relies on pickle, and TIL namedtuples do not play nice\n",
    "    # with pickle!\n",
    "    # Record = namedtuple('record', 'input_ids token_type_ids attention_mask y_first y_last')\n",
    "    # return Record(input_ids, token_type_ids, attention_mask, y_first, y_last)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"y_first\": y_first,\n",
    "        \"y_last\": y_last\n",
    "    }\n",
    "\n",
    "class TwitterSentimentExtractionDataset:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizers.BertWordPieceTokenizer(\n",
    "            f\"/mnt/bert-base-uncased/vocab.txt\", lowercase=True\n",
    "        )\n",
    "        self.max_len = 128\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return process_data(\n",
    "            self.df.text[item],\n",
    "            self.df.selected_text[item], \n",
    "            self.df.sentiment[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "        )\n",
    "\n",
    "# preprocessing pass; see comments in the previous code cell on why this is necessary\n",
    "X_train_preprocessing_pass = TwitterSentimentExtractionDataset(train)\n",
    "\n",
    "bad_idxs, good_idxs, y_firsts, y_lasts = [], [], [], []\n",
    "for i in range(len(train)):\n",
    "# for i in tqdm.tqdm(list(range(len(train)))):\n",
    "    try:\n",
    "        x = X_train_preprocessing_pass[i]\n",
    "        y_firsts.append(x['y_first'])\n",
    "        y_lasts.append(x['y_last'])\n",
    "        good_idxs.append(i)\n",
    "    except ValueError:\n",
    "        print(f\"Found bad record at idx {i}.\")\n",
    "        y_firsts.append(None)\n",
    "        y_lasts.append(None)\n",
    "        bad_idxs.append(i)\n",
    "\n",
    "del X_train_preprocessing_pass\n",
    "train_orig = train\n",
    "X_train_df = train.iloc[good_idxs].reset_index(drop=True)\n",
    "\n",
    "class TwitterSentimentExtractionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # configuring the model to output hidden states\n",
    "        cfg = transformers.PretrainedConfig.get_config_dict(\"bert-base-uncased\")[0]  # tuple?\n",
    "        cfg[\"output_hidden_states\"] = True\n",
    "        cfg = transformers.BertConfig.from_dict(cfg)\n",
    "        self.bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\", config=cfg)\n",
    "        \n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.l0 = nn.Linear(768 * 2, 2)\n",
    "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
    "        # self.out = nn.LogSoftmax(dim=-2)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # ignore the output from the model head, we'll instead we'll construct our own attention\n",
    "        # head connected to the last two layers of hidden weights.\n",
    "        # that's 512x762x2=780288 connections.\n",
    "        _, _, out = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        out = torch.cat((out[-1], out[-2]), dim=-1)\n",
    "        out = self.drop_out(out)\n",
    "        # the new head uses a linear layer with two output nodes.\n",
    "        # the first node learns sequence start.\n",
    "        # the second node learns sequence end.\n",
    "        logits = self.l0(out)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        \n",
    "        return start_logits, end_logits\n",
    "        \n",
    "        # TODO: embed softmax directly into the model arch\n",
    "        # y_start, y_end = self.out(start_logits), self.out(end_logits)\n",
    "        # return y_start, y_end\n",
    "\n",
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    y_pred_first_loss = ce(start_logits, start_positions)\n",
    "    y_pred_last_loss = ce(end_logits, end_positions)\n",
    "    y_pred_loss = (y_pred_first_loss + y_pred_last_loss)\n",
    "    return y_pred_loss\n",
    "\n",
    "# create model\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = TwitterSentimentExtractionModel()\n",
    "model.to(device)\n",
    "\n",
    "# create optimizer; uses weight decay\n",
    "model_parameters = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "    {'params': [p for n, p in model_parameters if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "    {'params': [p for n, p in model_parameters if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "\n",
    "# batch size and epochs globals; setting batch_size = 1 for testing, 64 is too much for the K80\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "# dataset and dataloader\n",
    "dataset = TwitterSentimentExtractionDataset(X_train_df)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "# create scheduler, this one is a transformers module hookup, thanks huggingface\n",
    "num_train_steps = int(len(dataloader) * epochs)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_train_steps\n",
    ")\n",
    "\n",
    "# train func for one epoch of training\n",
    "def train_fn(dataloader, model, optimizer, device, scheduler, epoch_num):\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    fn = lambda field: records[field].to(device, dtype=torch.long)\n",
    "    for idx, records in enumerate(dataloader):\n",
    "        # move the record to GPU\n",
    "        input_ids = fn(\"input_ids\")\n",
    "        token_type_ids = fn(\"token_type_ids\")\n",
    "        attention_mask = fn(\"attention_mask\")\n",
    "        y_first = fn(\"y_first\")\n",
    "        y_last = fn(\"y_last\")\n",
    "        \n",
    "        model.zero_grad()\n",
    "        y_pred_start_logits, y_pred_end_logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        loss = loss_fn(y_pred_start_logits, y_pred_end_logits, y_first, y_last)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        y_pred_starts = torch.softmax(y_pred_start_logits, dim=1).cpu().detach().numpy()\n",
    "        y_pred_ends = torch.softmax(y_pred_end_logits, dim=1).cpu().detach().numpy()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if idx % 10 == 0:\n",
    "            print(f\"epoch {epoch_num}, batch {idx} training loss: {losses[-1]}\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "def main():\n",
    "    checkpoints_dir = \"/spell/checkpoints/\"\n",
    "    if not os.path.exists(checkpoints_dir):\n",
    "        os.mkdir(checkpoints_dir)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_fn(dataloader, model, optimizer, device, scheduler, epoch)\n",
    "        torch.save(model.state_dict(), f\"/spell/checkpoints/model_{epoch}.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(\"/spell/checkpoints/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"/spell/checkpoints/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
