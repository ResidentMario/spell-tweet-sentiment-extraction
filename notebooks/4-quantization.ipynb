{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quantization\n",
    "\n",
    "This is an implementation of the model using dynamic quantization. This model is derived from `model_1.py`, *not* `model_2.py` (the former is the baseline model, the latter implements mixed precision training; we aren't doing _both_ quantization and mixed precision training here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd290a89d554a51b60e4d9f069fe542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=433.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052dd00e766945f284be33777cd63c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=440473133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "cfg = transformers.PretrainedConfig.get_config_dict(\"bert-base-uncased\")[0]  # tuple?\n",
    "cfg[\"output_hidden_states\"] = True\n",
    "cfg = transformers.BertConfig.from_dict(cfg)\n",
    "bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\", config=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "qbert = torch.quantization.quantize_dynamic(\n",
    "    bert, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the new `DynamicQuantizedLinear` layers in the model `repr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (key): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (value): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, scale=1.0, zero_point=0)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, scale=1.0, zero_point=0)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (key): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (value): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, scale=1.0, zero_point=0)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, scale=1.0, zero_point=0)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (key): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (value): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, scale=1.0, zero_point=0)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, scale=1.0, zero_point=0)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (key): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (value): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, scale=1.0, zero_point=0)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, scale=1.0, zero_point=0)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (key): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (value): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, scale=1.0, zero_point=0)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, scale=1.0, zero_point=0)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (key): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (value): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, scale=1.0, zero_point=0)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, scale=1.0, zero_point=0)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (key): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (value): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, scale=1.0, zero_point=0)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, scale=1.0, zero_point=0)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (key): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (value): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, scale=1.0, zero_point=0)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, scale=1.0, zero_point=0)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (key): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (value): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, scale=1.0, zero_point=0)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, scale=1.0, zero_point=0)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (key): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (value): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, scale=1.0, zero_point=0)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, scale=1.0, zero_point=0)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (key): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (value): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, scale=1.0, zero_point=0)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, scale=1.0, zero_point=0)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (key): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (value): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, scale=1.0, zero_point=0)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, scale=1.0, zero_point=0)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qbert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The run command is:\n",
    "\n",
    "```bash\n",
    "spell run \\\n",
    "  --machine-type t4 \\\n",
    "  --github-url https://github.com/spellml/tweet-sentiment-extraction.git \\\n",
    "  --pip transformers==3.5.1 --pip tokenizers --pip kaggle --pip tensorboard \\\n",
    "  --env KAGGLE_USERNAME=YOUR_USERNAME \\\n",
    "  --env KAGGLE_KEY=YOUR_KEY \\\n",
    "  --tensorboard-dir /spell/tensorboards/model_3 \\\n",
    "  \"chmod +x /spell/scripts/download_data.sh; /spell/scripts/download_data.sh; python /spell/models/model_2.py\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't even load a dynamically quantized model on a GPU, it turns out. You have to do so in a CPU context. The process seems to be to attach quantization to the model on the _target_ machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir ../servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../servers/eval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../servers/eval.py\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import time\n",
    "\n",
    "train = pd.read_csv(\"/mnt/tweet-sentiment-extraction/train.csv\")\n",
    "\n",
    "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
    "    # yes this really happens lol, try idx 314\n",
    "    if pd.isnull(tweet) or pd.isnull(selected_text) or len(tweet) == 0 or len(selected_text) == 0:\n",
    "        raise ValueError(\"text or selected_text is nan.\")\n",
    "    \n",
    "    # get indicial boundaries of substring.\n",
    "    target_char_idx_start = tweet.index(selected_text)\n",
    "    target_char_idx_end = target_char_idx_start + len(selected_text)\n",
    "\n",
    "    # build the character attention mask (used to build the token attention mask)\n",
    "    char_target_mask = (\n",
    "        [0] * target_char_idx_start +\n",
    "        [1] * (target_char_idx_end - target_char_idx_start) +\n",
    "        [0] * (len(tweet) - target_char_idx_end)\n",
    "    )\n",
    "    \n",
    "    # tokenize\n",
    "    # `ids` is the token values, `offsets` are the position tuples for the tokes in the str\n",
    "    tokens_obj = tokenizer.encode(tweet)\n",
    "    token_ids, token_offsets = tokens_obj.ids, tokens_obj.offsets\n",
    "    \n",
    "    # this is the clever bit. recall that the task is to find the subsequence in the sequence\n",
    "    # exemplifying the given sentiment. to do this we reformulate the input sequence as a\n",
    "    # question-answer pair, where the sentiment (as a single word) is the question and the\n",
    "    # sequence as a whole is the answer.\n",
    "    # \n",
    "    # this allows us to use version of the BERT model pretrained on the well-formed and\n",
    "    # well-studied question-answering task as a surrogate for this task.\n",
    "    sentiment_id_map = {\n",
    "        'positive': 3893,\n",
    "        'negative': 4997,\n",
    "        'neutral': 8699\n",
    "    }\n",
    "    # 101 is [CLS] and 102 is [SEP]. BERT expects Q/A input to be in the form\n",
    "    # [CLS] [...] [SEP] [...] [SEP]. Cf.\n",
    "    # https://huggingface.co/transformers/glossary.html#token-type-ids\n",
    "    # NOTE: the [-1:1] is the excise the start-of-seq and end-of-seq in the tokens\n",
    "    input_ids = [101] + [sentiment_id_map[sentiment]] + [102] + token_ids[1:-1] + [102]\n",
    "    \n",
    "    # BERT expects Q/A pairs to come with a binary mask splitting the pair types\n",
    "    # NOTE: the mafs excludes start-of-seq and end-of-seq but includes the new end-of-seq\n",
    "    token_type_ids = [0, 0, 0] + [1] * (len(token_ids) - 2 + 1)\n",
    "\n",
    "    # pad to max_len and create a corresponding attention mask\n",
    "    pad_len = max_len - len(input_ids)\n",
    "    attention_mask = [1] * len(input_ids) + [0] * pad_len\n",
    "    input_ids = input_ids + [0] * pad_len\n",
    "    token_type_ids = token_type_ids + [0] * pad_len\n",
    "    \n",
    "    # get the index of the first and last token of the target, this is what the model will try\n",
    "    # to predict! see the notes on the head layer in forward for more info.\n",
    "    # we add 3 because the first thee elements of the mask are always [CLS] $SENTIMENT [CLS]\n",
    "    # and always get an attention vector [1 1 1].\n",
    "    ufunc = lambda first, _: first >= target_char_idx_start and first < target_char_idx_end\n",
    "    y_pred_mask = [ufunc(*offset) for offset in token_offsets]\n",
    "    try:\n",
    "        y_first = 3 + y_pred_mask.index(True)\n",
    "        y_last = 3 + len(y_pred_mask) - y_pred_mask[::-1].index(True) - 1\n",
    "    except ValueError:\n",
    "        # some of the labels are noisy, and the first character in the label does not actually\n",
    "        # correspond with the first character of any token (e.g. the label is a part-of-a-word\n",
    "        # instead of a word). I'm going to venture the opinion here that these records \n",
    "        # constitute data noise (because, I mean, they are) and should be removed in\n",
    "        # pre-processing\n",
    "        raise ValueError(\n",
    "            f\"Found bad selected_text value '{selected_text}' for tweet '{tweet}'.\"\n",
    "            f\"Make sure to get rid of these in a pre-processing pass.\"\n",
    "        )\n",
    "    \n",
    "    # convert to torch tensors\n",
    "    t = lambda seq: torch.tensor(seq, dtype=torch.long)\n",
    "    input_ids, token_type_ids, attention_mask, y_first, y_last =\\\n",
    "        t(input_ids), t(token_type_ids), t(attention_mask), t(y_first), t(y_last)\n",
    "    \n",
    "    # output\n",
    "    # Unfortunately the PyTorch dataloader relies on pickle, and TIL namedtuples do not play nice\n",
    "    # with pickle!\n",
    "    # Record = namedtuple('record', 'input_ids token_type_ids attention_mask y_first y_last')\n",
    "    # return Record(input_ids, token_type_ids, attention_mask, y_first, y_last)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"y_first\": y_first,\n",
    "        \"y_last\": y_last\n",
    "    }\n",
    "\n",
    "class TwitterSentimentExtractionDataset:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizers.BertWordPieceTokenizer(\n",
    "            f\"/mnt/bert-base-uncased/vocab.txt\", lowercase=True\n",
    "        )\n",
    "        self.max_len = 128\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return process_data(\n",
    "            self.df.text[item],\n",
    "            self.df.selected_text[item], \n",
    "            self.df.sentiment[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "        )\n",
    "\n",
    "# preprocessing pass; see comments in the previous code cell on why this is necessary\n",
    "X_train_preprocessing_pass = TwitterSentimentExtractionDataset(train)\n",
    "\n",
    "bad_idxs, good_idxs, y_firsts, y_lasts = [], [], [], []\n",
    "for i in range(len(train)):\n",
    "    try:\n",
    "        x = X_train_preprocessing_pass[i]\n",
    "        y_firsts.append(x['y_first'])\n",
    "        y_lasts.append(x['y_last'])\n",
    "        good_idxs.append(i)\n",
    "    except ValueError:\n",
    "        print(f\"Found bad record at idx {i}.\")\n",
    "        y_firsts.append(None)\n",
    "        y_lasts.append(None)\n",
    "        bad_idxs.append(i)\n",
    "\n",
    "del X_train_preprocessing_pass\n",
    "train_orig = train\n",
    "X_train_df = train.iloc[good_idxs].reset_index(drop=True)\n",
    "\n",
    "class TwitterSentimentExtractionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # configuring the model to output hidden states\n",
    "        cfg = transformers.PretrainedConfig.get_config_dict(\"bert-base-uncased\")[0]  # tuple?\n",
    "        cfg[\"output_hidden_states\"] = True\n",
    "        cfg = transformers.BertConfig.from_dict(cfg)\n",
    "        \n",
    "        bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\", config=cfg)\n",
    "        self.bert = bert\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.l0 = nn.Linear(768 * 2, 2)\n",
    "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
    "        # self.out = nn.LogSoftmax(dim=-2)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # ignore the output from the model head, we'll instead we'll construct our own attention\n",
    "        # head connected to the last two layers of hidden weights.\n",
    "        # that's 512x762x2=780288 connections.\n",
    "        _, _, out = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        out = torch.cat((out[-1], out[-2]), dim=-1)\n",
    "        out = self.drop_out(out)\n",
    "        # the new head uses a linear layer with two output nodes.\n",
    "        # the first node learns sequence start.\n",
    "        # the second node learns sequence end.\n",
    "        logits = self.l0(out)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        \n",
    "        return start_logits, end_logits\n",
    "        \n",
    "        # TODO: embed softmax directly into the model arch\n",
    "        # y_start, y_end = self.out(start_logits), self.out(end_logits)\n",
    "        # return y_start, y_end\n",
    "\n",
    "# constants\n",
    "batch_size = 64\n",
    "if torch.cuda.device_count() >= 1:\n",
    "    device = torch.device(\"cuda\")\n",
    "    is_cpu_run = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    is_cpu_run = True\n",
    "\n",
    "# dataset and dataloader\n",
    "dataset = TwitterSentimentExtractionDataset(X_train_df)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "# eval func for one epoch of evaluation\n",
    "def eval_fn(dataloader, model, device):\n",
    "    model.eval()\n",
    "    \n",
    "    fn = lambda field: records[field].to(device, dtype=torch.long)\n",
    "    for idx, records in enumerate(dataloader):\n",
    "        # move the record to GPU\n",
    "        input_ids = fn(\"input_ids\")\n",
    "        token_type_ids = fn(\"token_type_ids\")\n",
    "        attention_mask = fn(\"attention_mask\")\n",
    "        y_first = fn(\"y_first\")\n",
    "        y_last = fn(\"y_last\")\n",
    "        \n",
    "        y_pred_start_logits, y_pred_end_logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        y_pred_starts = torch.softmax(y_pred_start_logits, dim=1).cpu().detach().numpy()\n",
    "        y_pred_ends = torch.softmax(y_pred_end_logits, dim=1).cpu().detach().numpy()\n",
    "        return y_pred_starts, y_pred_ends\n",
    "\n",
    "\n",
    "def main():\n",
    "    checkpoints_dir = \"/spell/checkpoints\"\n",
    "    model = TwitterSentimentExtractionModel()\n",
    "    \n",
    "    if is_cpu_run:\n",
    "        weights = torch.load(f\"{checkpoints_dir}/model_5.pth\", map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        weights = torch.load(f\"{checkpoints_dir}/model_5.pth\")\n",
    "\n",
    "    model.load_state_dict(weights)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Evaluating the model...\")\n",
    "    start_time = time.time()\n",
    "    eval_fn(dataloader, model, device)\n",
    "    print(f\"Evaluation done in {str(time.time() - start_time)} seconds.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "spell run \\\n",
    "  --machine-type t4 \\\n",
    "  --github-url https://github.com/spellml/tweet-sentiment-extraction.git \\\n",
    "  --pip transformers==3.5.1 --pip tokenizers --pip kaggle \\\n",
    "  --env KAGGLE_USERNAME=YOUR_USERNAME \\\n",
    "  --env KAGGLE_KEY=YOUR_KEY \\\n",
    "  --mount runs/414/checkpoints/model_5.pth:/spell/checkpoints/model_5.pth \\\n",
    "  \"chmod +x /spell/scripts/download_data.sh; /spell/scripts/download_data.sh; python /spell/servers/eval.py\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../servers/eval_quantized.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../servers/eval_quantized.py\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import time\n",
    "\n",
    "train = pd.read_csv(\"/mnt/tweet-sentiment-extraction/train.csv\")\n",
    "\n",
    "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
    "    # yes this really happens lol, try idx 314\n",
    "    if pd.isnull(tweet) or pd.isnull(selected_text) or len(tweet) == 0 or len(selected_text) == 0:\n",
    "        raise ValueError(\"text or selected_text is nan.\")\n",
    "    \n",
    "    # get indicial boundaries of substring.\n",
    "    target_char_idx_start = tweet.index(selected_text)\n",
    "    target_char_idx_end = target_char_idx_start + len(selected_text)\n",
    "\n",
    "    # build the character attention mask (used to build the token attention mask)\n",
    "    char_target_mask = (\n",
    "        [0] * target_char_idx_start +\n",
    "        [1] * (target_char_idx_end - target_char_idx_start) +\n",
    "        [0] * (len(tweet) - target_char_idx_end)\n",
    "    )\n",
    "    \n",
    "    # tokenize\n",
    "    # `ids` is the token values, `offsets` are the position tuples for the tokes in the str\n",
    "    tokens_obj = tokenizer.encode(tweet)\n",
    "    token_ids, token_offsets = tokens_obj.ids, tokens_obj.offsets\n",
    "    \n",
    "    # this is the clever bit. recall that the task is to find the subsequence in the sequence\n",
    "    # exemplifying the given sentiment. to do this we reformulate the input sequence as a\n",
    "    # question-answer pair, where the sentiment (as a single word) is the question and the\n",
    "    # sequence as a whole is the answer.\n",
    "    # \n",
    "    # this allows us to use version of the BERT model pretrained on the well-formed and\n",
    "    # well-studied question-answering task as a surrogate for this task.\n",
    "    sentiment_id_map = {\n",
    "        'positive': 3893,\n",
    "        'negative': 4997,\n",
    "        'neutral': 8699\n",
    "    }\n",
    "    # 101 is [CLS] and 102 is [SEP]. BERT expects Q/A input to be in the form\n",
    "    # [CLS] [...] [SEP] [...] [SEP]. Cf.\n",
    "    # https://huggingface.co/transformers/glossary.html#token-type-ids\n",
    "    # NOTE: the [-1:1] is the excise the start-of-seq and end-of-seq in the tokens\n",
    "    input_ids = [101] + [sentiment_id_map[sentiment]] + [102] + token_ids[1:-1] + [102]\n",
    "    \n",
    "    # BERT expects Q/A pairs to come with a binary mask splitting the pair types\n",
    "    # NOTE: the mafs excludes start-of-seq and end-of-seq but includes the new end-of-seq\n",
    "    token_type_ids = [0, 0, 0] + [1] * (len(token_ids) - 2 + 1)\n",
    "\n",
    "    # pad to max_len and create a corresponding attention mask\n",
    "    pad_len = max_len - len(input_ids)\n",
    "    attention_mask = [1] * len(input_ids) + [0] * pad_len\n",
    "    input_ids = input_ids + [0] * pad_len\n",
    "    token_type_ids = token_type_ids + [0] * pad_len\n",
    "    \n",
    "    # get the index of the first and last token of the target, this is what the model will try\n",
    "    # to predict! see the notes on the head layer in forward for more info.\n",
    "    # we add 3 because the first thee elements of the mask are always [CLS] $SENTIMENT [CLS]\n",
    "    # and always get an attention vector [1 1 1].\n",
    "    ufunc = lambda first, _: first >= target_char_idx_start and first < target_char_idx_end\n",
    "    y_pred_mask = [ufunc(*offset) for offset in token_offsets]\n",
    "    try:\n",
    "        y_first = 3 + y_pred_mask.index(True)\n",
    "        y_last = 3 + len(y_pred_mask) - y_pred_mask[::-1].index(True) - 1\n",
    "    except ValueError:\n",
    "        # some of the labels are noisy, and the first character in the label does not actually\n",
    "        # correspond with the first character of any token (e.g. the label is a part-of-a-word\n",
    "        # instead of a word). I'm going to venture the opinion here that these records \n",
    "        # constitute data noise (because, I mean, they are) and should be removed in\n",
    "        # pre-processing\n",
    "        raise ValueError(\n",
    "            f\"Found bad selected_text value '{selected_text}' for tweet '{tweet}'.\"\n",
    "            f\"Make sure to get rid of these in a pre-processing pass.\"\n",
    "        )\n",
    "    \n",
    "    # convert to torch tensors\n",
    "    t = lambda seq: torch.tensor(seq, dtype=torch.long)\n",
    "    input_ids, token_type_ids, attention_mask, y_first, y_last =\\\n",
    "        t(input_ids), t(token_type_ids), t(attention_mask), t(y_first), t(y_last)\n",
    "    \n",
    "    # output\n",
    "    # Unfortunately the PyTorch dataloader relies on pickle, and TIL namedtuples do not play nice\n",
    "    # with pickle!\n",
    "    # Record = namedtuple('record', 'input_ids token_type_ids attention_mask y_first y_last')\n",
    "    # return Record(input_ids, token_type_ids, attention_mask, y_first, y_last)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"y_first\": y_first,\n",
    "        \"y_last\": y_last\n",
    "    }\n",
    "\n",
    "class TwitterSentimentExtractionDataset:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizers.BertWordPieceTokenizer(\n",
    "            f\"/mnt/bert-base-uncased/vocab.txt\", lowercase=True\n",
    "        )\n",
    "        self.max_len = 128\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return process_data(\n",
    "            self.df.text[item],\n",
    "            self.df.selected_text[item], \n",
    "            self.df.sentiment[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "        )\n",
    "\n",
    "# preprocessing pass; see comments in the previous code cell on why this is necessary\n",
    "X_train_preprocessing_pass = TwitterSentimentExtractionDataset(train)\n",
    "\n",
    "bad_idxs, good_idxs, y_firsts, y_lasts = [], [], [], []\n",
    "for i in range(len(train)):\n",
    "    try:\n",
    "        x = X_train_preprocessing_pass[i]\n",
    "        y_firsts.append(x['y_first'])\n",
    "        y_lasts.append(x['y_last'])\n",
    "        good_idxs.append(i)\n",
    "    except ValueError:\n",
    "        print(f\"Found bad record at idx {i}.\")\n",
    "        y_firsts.append(None)\n",
    "        y_lasts.append(None)\n",
    "        bad_idxs.append(i)\n",
    "\n",
    "del X_train_preprocessing_pass\n",
    "train_orig = train\n",
    "X_train_df = train.iloc[good_idxs].reset_index(drop=True)\n",
    "\n",
    "class TwitterSentimentExtractionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # configuring the model to output hidden states\n",
    "        cfg = transformers.PretrainedConfig.get_config_dict(\"bert-base-uncased\")[0]  # tuple?\n",
    "        cfg[\"output_hidden_states\"] = True\n",
    "        cfg = transformers.BertConfig.from_dict(cfg)\n",
    "        \n",
    "        bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\", config=cfg)\n",
    "        self.bert = bert\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.l0 = nn.Linear(768 * 2, 2)\n",
    "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
    "        # self.out = nn.LogSoftmax(dim=-2)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # ignore the output from the model head, we'll instead we'll construct our own attention\n",
    "        # head connected to the last two layers of hidden weights.\n",
    "        # that's 512x762x2=780288 connections.\n",
    "        _, _, out = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        out = torch.cat((out[-1], out[-2]), dim=-1)\n",
    "        out = self.drop_out(out)\n",
    "        # the new head uses a linear layer with two output nodes.\n",
    "        # the first node learns sequence start.\n",
    "        # the second node learns sequence end.\n",
    "        logits = self.l0(out)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        \n",
    "        return start_logits, end_logits\n",
    "        \n",
    "        # TODO: embed softmax directly into the model arch\n",
    "        # y_start, y_end = self.out(start_logits), self.out(end_logits)\n",
    "        # return y_start, y_end\n",
    "\n",
    "# constants\n",
    "batch_size = 64\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# dataset and dataloader\n",
    "dataset = TwitterSentimentExtractionDataset(X_train_df)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "# eval func for one epoch of evaluation\n",
    "def eval_fn(dataloader, model, device):\n",
    "    model.eval()\n",
    "    \n",
    "    fn = lambda field: records[field].to(device, dtype=torch.long)\n",
    "    for idx, records in enumerate(dataloader):\n",
    "        # move the record to GPU\n",
    "        input_ids = fn(\"input_ids\")\n",
    "        token_type_ids = fn(\"token_type_ids\")\n",
    "        attention_mask = fn(\"attention_mask\")\n",
    "        y_first = fn(\"y_first\")\n",
    "        y_last = fn(\"y_last\")\n",
    "        \n",
    "        y_pred_start_logits, y_pred_end_logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        y_pred_starts = torch.softmax(y_pred_start_logits, dim=1).cpu().detach().numpy()\n",
    "        y_pred_ends = torch.softmax(y_pred_end_logits, dim=1).cpu().detach().numpy()\n",
    "        return y_pred_starts, y_pred_ends\n",
    "\n",
    "\n",
    "def main():\n",
    "    checkpoints_dir = \"/spell/checkpoints\"\n",
    "    model = TwitterSentimentExtractionModel()\n",
    "    model.load_state_dict(torch.load(f\"{checkpoints_dir}/model_5.pth\", map_location=torch.device('cpu')))\n",
    "    qmodel = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "    qmodel.to(device)\n",
    "    qmodel.eval()\n",
    "    \n",
    "    print(f\"Evaluating the model...\")\n",
    "    start_time = time.time()\n",
    "    eval_fn(dataloader, qmodel, device)\n",
    "    print(f\"Evaluation done in {str(time.time() - start_time)} seconds.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Localizing the data for development purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==3.5.1\n",
      "  Downloading transformers-3.5.1-py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 8.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (3.13.0)\n",
      "Collecting tokenizers==0.9.3\n",
      "  Downloading tokenizers-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 31.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (4.51.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (1.16.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (20.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (0.0.43)\n",
      "Collecting sentencepiece==0.1.91\n",
      "  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 64.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (2020.11.13)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (2.25.0)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers==3.5.1) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers==3.5.1) (50.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.5.1) (2.4.7)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.1) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.1) (0.17.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (1.26.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (2020.11.8)\n",
      "Installing collected packages: tokenizers, sentencepiece, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.9.4\n",
      "    Uninstalling tokenizers-0.9.4:\n",
      "      Successfully uninstalled tokenizers-0.9.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.0.0\n",
      "    Uninstalling transformers-4.0.0:\n",
      "      Successfully uninstalled transformers-4.0.0\n",
      "Successfully installed sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers==3.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating an example model server input\n",
    "\n",
    "In this section I go about generating an example input for the model server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweet-sentiment-extraction.zip to /spell/notebooks\n",
      "\n",
      "Archive:  tweet-sentiment-extraction.zip\n",
      "  inflating: /mnt/tweet-sentiment-extraction/sample_submission.csv  \n",
      "  inflating: /mnt/tweet-sentiment-extraction/test.csv  \n",
      "  inflating: /mnt/tweet-sentiment-extraction/train.csv  \n",
      "Downloading bert-base-uncased.zip to /spell/notebooks\n",
      "\n",
      "Archive:  bert-base-uncased.zip\n",
      "  inflating: /mnt/bert-base-uncased/config.json  \n",
      "  inflating: /mnt/bert-base-uncased/pytorch_model.bin  \n",
      "  inflating: /mnt/bert-base-uncased/vocab.txt  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.39M/1.39M [00:00<00:00, 16.8MB/s]\n",
      "100%|██████████| 389M/389M [00:06<00:00, 59.9MB/s] \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export KAGGLE_USERNAME=residentmario\n",
    "export KAGGLE_KEY=5ae93199f0523afe441e922fcdc75ee4\n",
    "chmod +x /spell/scripts/download_data.sh; /spell/scripts/download_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "train = pd.read_csv(\"/mnt/tweet-sentiment-extraction/train.csv\")\n",
    "\n",
    "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
    "    # yes this really happens lol, try idx 314\n",
    "    if pd.isnull(tweet) or pd.isnull(selected_text) or len(tweet) == 0 or len(selected_text) == 0:\n",
    "        raise ValueError(\"text or selected_text is nan.\")\n",
    "    \n",
    "    # get indicial boundaries of substring.\n",
    "    target_char_idx_start = tweet.index(selected_text)\n",
    "    target_char_idx_end = target_char_idx_start + len(selected_text)\n",
    "\n",
    "    # build the character attention mask (used to build the token attention mask)\n",
    "    char_target_mask = (\n",
    "        [0] * target_char_idx_start +\n",
    "        [1] * (target_char_idx_end - target_char_idx_start) +\n",
    "        [0] * (len(tweet) - target_char_idx_end)\n",
    "    )\n",
    "    \n",
    "    # tokenize\n",
    "    # `ids` is the token values, `offsets` are the position tuples for the tokes in the str\n",
    "    tokens_obj = tokenizer.encode(tweet)\n",
    "    token_ids, token_offsets = tokens_obj.ids, tokens_obj.offsets\n",
    "    \n",
    "    # this is the clever bit. recall that the task is to find the subsequence in the sequence\n",
    "    # exemplifying the given sentiment. to do this we reformulate the input sequence as a\n",
    "    # question-answer pair, where the sentiment (as a single word) is the question and the\n",
    "    # sequence as a whole is the answer.\n",
    "    # \n",
    "    # this allows us to use version of the BERT model pretrained on the well-formed and\n",
    "    # well-studied question-answering task as a surrogate for this task.\n",
    "    sentiment_id_map = {\n",
    "        'positive': 3893,\n",
    "        'negative': 4997,\n",
    "        'neutral': 8699\n",
    "    }\n",
    "    # 101 is [CLS] and 102 is [SEP]. BERT expects Q/A input to be in the form\n",
    "    # [CLS] [...] [SEP] [...] [SEP]. Cf.\n",
    "    # https://huggingface.co/transformers/glossary.html#token-type-ids\n",
    "    # NOTE: the [-1:1] is the excise the start-of-seq and end-of-seq in the tokens\n",
    "    input_ids = [101] + [sentiment_id_map[sentiment]] + [102] + token_ids[1:-1] + [102]\n",
    "    \n",
    "    # BERT expects Q/A pairs to come with a binary mask splitting the pair types\n",
    "    # NOTE: the mafs excludes start-of-seq and end-of-seq but includes the new end-of-seq\n",
    "    token_type_ids = [0, 0, 0] + [1] * (len(token_ids) - 2 + 1)\n",
    "\n",
    "    # pad to max_len and create a corresponding attention mask\n",
    "    pad_len = max_len - len(input_ids)\n",
    "    attention_mask = [1] * len(input_ids) + [0] * pad_len\n",
    "    input_ids = input_ids + [0] * pad_len\n",
    "    token_type_ids = token_type_ids + [0] * pad_len\n",
    "    \n",
    "    # get the index of the first and last token of the target, this is what the model will try\n",
    "    # to predict! see the notes on the head layer in forward for more info.\n",
    "    # we add 3 because the first thee elements of the mask are always [CLS] $SENTIMENT [CLS]\n",
    "    # and always get an attention vector [1 1 1].\n",
    "    ufunc = lambda first, _: first >= target_char_idx_start and first < target_char_idx_end\n",
    "    y_pred_mask = [ufunc(*offset) for offset in token_offsets]\n",
    "    try:\n",
    "        y_first = 3 + y_pred_mask.index(True)\n",
    "        y_last = 3 + len(y_pred_mask) - y_pred_mask[::-1].index(True) - 1\n",
    "    except ValueError:\n",
    "        # some of the labels are noisy, and the first character in the label does not actually\n",
    "        # correspond with the first character of any token (e.g. the label is a part-of-a-word\n",
    "        # instead of a word). I'm going to venture the opinion here that these records \n",
    "        # constitute data noise (because, I mean, they are) and should be removed in\n",
    "        # pre-processing\n",
    "        raise ValueError(\n",
    "            f\"Found bad selected_text value '{selected_text}' for tweet '{tweet}'.\"\n",
    "            f\"Make sure to get rid of these in a pre-processing pass.\"\n",
    "        )\n",
    "    \n",
    "    # convert to torch tensors\n",
    "    t = lambda seq: torch.tensor(seq, dtype=torch.long)\n",
    "    input_ids, token_type_ids, attention_mask, y_first, y_last =\\\n",
    "        t(input_ids), t(token_type_ids), t(attention_mask), t(y_first), t(y_last)\n",
    "    \n",
    "    # output\n",
    "    # Unfortunately the PyTorch dataloader relies on pickle, and TIL namedtuples do not play nice\n",
    "    # with pickle!\n",
    "    # Record = namedtuple('record', 'input_ids token_type_ids attention_mask y_first y_last')\n",
    "    # return Record(input_ids, token_type_ids, attention_mask, y_first, y_last)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"y_first\": y_first,\n",
    "        \"y_last\": y_last\n",
    "    }\n",
    "\n",
    "class TwitterSentimentExtractionDataset:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizers.BertWordPieceTokenizer(\n",
    "            f\"/mnt/bert-base-uncased/vocab.txt\", lowercase=True\n",
    "        )\n",
    "        self.max_len = 128\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return process_data(\n",
    "            self.df.text[item],\n",
    "            self.df.selected_text[item], \n",
    "            self.df.sentiment[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TwitterSentimentExtractionDataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 101, 8699,  102, 1045, 1036, 1040, 2031, 5838, 1010, 2065, 1045, 2020,\n",
       "         2183,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'y_first': tensor(4),\n",
       " 'y_last': tensor(13)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = next(iter(ds))\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_fmt = dict()\n",
    "ex_fmt['input_ids'] = ex['input_ids'].tolist()\n",
    "ex_fmt['token_type_ids'] = ex['token_type_ids'].tolist()\n",
    "ex_fmt['attention_mask'] = ex['attention_mask'].tolist()\n",
    "ex_fmt['y_first'] = ex['y_first'].item()\n",
    "ex_fmt['y_last'] = ex['y_last'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0],\n",
      "'input_ids': [101, 8699, 102, 1045, 1036, 1040, 2031, 5838, 1010, 2065, 1045,\n",
      "             2020, 2183, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "             0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "'token_type_ids': [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                  0, 0],\n",
      "'y_first': 4,\n",
      "'y_last': 13}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=0, compact=True)\n",
    "pp.pprint(ex_fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../servers/example_payload.json\", \"w\") as fp:\n",
    "    json.dump(ex_fmt, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    ds,\n",
    "    batch_size=1,\n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 8699,  102, 1045, 1036, 1040, 2031, 5838, 1010, 2065, 1045, 2020,\n",
       "          2183,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'y_first': tensor([4]),\n",
       " 'y_last': tensor([13])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([ex_fmt['input_ids']])\n",
    "token_type_ids = torch.tensor([ex_fmt['token_type_ids']])\n",
    "attention_mask = torch.tensor([ex_fmt['attention_mask']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterSentimentExtractionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # configuring the model to output hidden states\n",
    "        cfg = transformers.PretrainedConfig.get_config_dict(\"bert-base-uncased\")[0]  # tuple?\n",
    "        cfg[\"output_hidden_states\"] = True\n",
    "        cfg = transformers.BertConfig.from_dict(cfg)\n",
    "        \n",
    "        bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\", config=cfg)\n",
    "        self.bert = bert\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.l0 = nn.Linear(768 * 2, 2)\n",
    "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
    "        # self.out = nn.LogSoftmax(dim=-2)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # ignore the output from the model head, we'll instead we'll construct our own attention\n",
    "        # head connected to the last two layers of hidden weights.\n",
    "        # that's 512x762x2=780288 connections.\n",
    "        _, _, out = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        out = torch.cat((out[-1], out[-2]), dim=-1)\n",
    "        out = self.drop_out(out)\n",
    "        # the new head uses a linear layer with two output nodes.\n",
    "        # the first node learns sequence start.\n",
    "        # the second node learns sequence end.\n",
    "        logits = self.l0(out)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        \n",
    "        return start_logits, end_logits\n",
    "        \n",
    "        # TODO: embed softmax directly into the model arch\n",
    "        # y_start, y_end = self.out(start_logits), self.out(end_logits)\n",
    "        # return y_start, y_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a545d33b0142fda74ed7c5c0d24fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=433.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe1b34b4254c4b99b515041c4de74388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=440473133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = TwitterSentimentExtractionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3959, -0.1834,  0.1791,  0.6216,  0.0544, -0.3758, -0.5169,  0.4187,\n",
       "          -0.2306, -0.4681,  0.4183,  0.2476,  0.2307, -0.0617,  0.5199,  0.8549,\n",
       "           0.2622,  1.0879,  0.8102,  1.0117,  1.0577,  1.0207,  1.1073,  1.1544,\n",
       "           1.0915,  0.8919,  0.7396,  0.3420,  0.8595,  0.7967,  0.8687,  1.0728,\n",
       "           0.7715,  1.0828,  1.0236,  1.1534,  0.8812,  0.8963,  0.8666,  0.9138,\n",
       "           0.2778,  0.7737,  0.7584,  1.0188,  1.1032,  1.0181,  1.1179,  1.2806,\n",
       "           1.0602,  0.9708,  1.1679,  0.9774,  1.1420,  0.6806,  0.7948,  0.8754,\n",
       "           1.0770,  0.7149,  0.9675,  0.7816,  0.8900,  1.2835,  1.1273,  1.2192,\n",
       "          -0.4279,  0.9868,  1.0101,  1.1320,  1.0774,  1.4328,  0.6311,  0.9705,\n",
       "           1.3023,  1.1205,  1.0002,  1.1522,  1.3430,  0.9718,  0.7316,  0.9448,\n",
       "           0.9305,  1.0479,  0.9860,  0.9676,  0.8448,  0.9865,  1.0748,  0.6102,\n",
       "           1.0593,  0.9066,  0.7934,  0.9831,  0.9595,  0.0537,  0.4025,  0.8291,\n",
       "           0.4488,  0.6926,  0.5088,  0.3710,  1.2040,  1.1824,  0.6976,  0.9501,\n",
       "           0.8624,  0.9946,  0.8377,  0.2327,  1.0683,  0.8596,  0.8667,  0.8715,\n",
       "           0.8799,  0.8140,  0.9764,  0.8850,  0.6545,  1.0027,  0.9526,  1.2311,\n",
       "           1.0375,  1.1941,  0.7556,  0.8194,  0.8580,  1.0962,  0.7492,  0.9951]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([[-0.1569, -0.4623,  0.0856, -0.1470,  0.9683,  0.8351,  0.4013,  0.1693,\n",
       "          -0.1464,  0.1058, -0.0265,  0.3172, -0.4683,  0.0903, -0.7599, -0.5376,\n",
       "          -0.6206, -0.5935, -0.4419, -0.5409, -0.2779, -0.5073, -0.4603, -0.2786,\n",
       "          -0.2546, -0.0849, -0.6432,  0.3508, -0.2174, -0.1696, -0.0933, -0.0071,\n",
       "          -0.1912, -0.3908,  0.3357, -0.4614, -0.6886, -0.2203, -0.4134, -0.5181,\n",
       "          -0.1895, -0.1332, -0.7189, -0.2874, -0.3752, -0.6337, -0.2765, -0.4141,\n",
       "          -0.3131, -0.5733, -0.3264, -0.1776, -0.2828, -0.3730, -0.0178, -0.1591,\n",
       "          -0.3736, -0.2127, -0.4832, -0.1322, -0.5056, -0.1977,  0.0377, -0.9653,\n",
       "           0.3712, -0.8800, -0.3382, -0.6265, -0.2847, -0.3342, -0.3944,  0.0016,\n",
       "          -0.5836, -0.2642, -0.1671, -0.1869, -0.3716, -0.1926, -0.4297, -0.4640,\n",
       "          -0.6555, -0.1447, -0.3693, -0.4964,  0.2299, -0.3924, -0.2539, -0.4223,\n",
       "          -0.5585, -0.2245, -0.6279, -0.0616, -0.4067,  0.2861, -0.3581,  0.0545,\n",
       "           0.4781, -0.5852, -0.4180,  0.2767, -0.4966, -0.4937,  0.1185, -0.2197,\n",
       "          -0.0974, -0.3220, -0.5974,  0.1993, -0.2038, -0.1560, -0.2986, -0.2116,\n",
       "           0.1117, -0.2273, -0.3754, -0.6455, -0.2309, -0.1784, -0.4190, -0.5676,\n",
       "          -0.2705, -0.6329, -0.5281, -0.9222, -0.8396, -0.8473, -0.4512, -0.3702]],\n",
       "        grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\n",
    "    input_ids=input_ids,\n",
    "    token_type_ids=token_type_ids,\n",
    "    attention_mask=attention_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'attention_mask': \n",
    "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0],\n",
    "'input_ids':\n",
    "[101, 8699, 102, 1045, 1036, 1040, 2031, 5838, 1010, 2065, 1045,\n",
    " 2020, 2183, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "'token_type_ids':\n",
    "[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0],\n",
    "'y_first': 4,\n",
    "'y_last': 13}\n",
    "input_ids = torch.tensor([payload['input_ids']])\n",
    "token_type_ids = torch.tensor([payload['token_type_ids']])\n",
    "attention_mask = torch.tensor([payload['attention_mask']])\n",
    "start_logits, end_logits = model(\n",
    "input_ids=input_ids,\n",
    "attention_mask=attention_mask,\n",
    "token_type_ids=token_type_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([0])\n",
    "t.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.serialization.load(f, map_location=None, pickle_module=<module 'pickle' from '/usr/lib/python3.7/pickle.py'>, **pickle_load_args)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
