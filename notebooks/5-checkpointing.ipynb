{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('test-env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f66f011e3f3330f8e643e24c7d074e09c31bde4d0241cb804ea20473d020a1e2"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# checkpointing\n",
    "\n",
    "This is an implementation of the model using checkpointing. Checkpointing is built into the `transformers` API directly, so you only need to flip a single boolean flag to get it, pretty cool!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing ../models/model_5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../models/model_5.py\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "train = pd.read_csv(\"/mnt/tweet-sentiment-extraction/train.csv\")\n",
    "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
    "    # yes this really happens lol, try idx 314\n",
    "    if pd.isnull(tweet) or pd.isnull(selected_text) or len(tweet) == 0 or len(selected_text) == 0:\n",
    "        raise ValueError(\"text or selected_text is nan.\")\n",
    "    \n",
    "    # get indicial boundaries of substring.\n",
    "    target_char_idx_start = tweet.index(selected_text)\n",
    "    target_char_idx_end = target_char_idx_start + len(selected_text)\n",
    "\n",
    "    # build the character attention mask (used to build the token attention mask)\n",
    "    char_target_mask = (\n",
    "        [0] * target_char_idx_start +\n",
    "        [1] * (target_char_idx_end - target_char_idx_start) +\n",
    "        [0] * (len(tweet) - target_char_idx_end)\n",
    "    )\n",
    "    \n",
    "    # tokenize\n",
    "    # `ids` is the token values, `offsets` are the position tuples for the tokes in the str\n",
    "    tokens_obj = tokenizer.encode(tweet)\n",
    "    token_ids, token_offsets = tokens_obj.ids, tokens_obj.offsets\n",
    "    \n",
    "    # this is the clever bit. recall that the task is to find the subsequence in the sequence\n",
    "    # exemplifying the given sentiment. to do this we reformulate the input sequence as a\n",
    "    # question-answer pair, where the sentiment (as a single word) is the question and the\n",
    "    # sequence as a whole is the answer.\n",
    "    # \n",
    "    # this allows us to use version of the BERT model pretrained on the well-formed and\n",
    "    # well-studied question-answering task as a surrogate for this task.\n",
    "    sentiment_id_map = {\n",
    "        'positive': 3893,\n",
    "        'negative': 4997,\n",
    "        'neutral': 8699\n",
    "    }\n",
    "    # 101 is [CLS] and 102 is [SEP]. BERT expects Q/A input to be in the form\n",
    "    # [CLS] [...] [SEP] [...] [SEP]. Cf.\n",
    "    # https://huggingface.co/transformers/glossary.html#token-type-ids\n",
    "    # NOTE: the [-1:1] is the excise the start-of-seq and end-of-seq in the tokens\n",
    "    input_ids = [101] + [sentiment_id_map[sentiment]] + [102] + token_ids[1:-1] + [102]\n",
    "    \n",
    "    # BERT expects Q/A pairs to come with a binary mask splitting the pair types\n",
    "    # NOTE: the mafs excludes start-of-seq and end-of-seq but includes the new end-of-seq\n",
    "    token_type_ids = [0, 0, 0] + [1] * (len(token_ids) - 2 + 1)\n",
    "\n",
    "    # pad to max_len and create a corresponding attention mask\n",
    "    pad_len = max_len - len(input_ids)\n",
    "    attention_mask = [1] * len(input_ids) + [0] * pad_len\n",
    "    input_ids = input_ids + [0] * pad_len\n",
    "    token_type_ids = token_type_ids + [0] * pad_len\n",
    "    \n",
    "    # get the index of the first and last token of the target, this is what the model will try\n",
    "    # to predict! see the notes on the head layer in forward for more info.\n",
    "    # we add 3 because the first thee elements of the mask are always [CLS] $SENTIMENT [CLS]\n",
    "    # and always get an attention vector [1 1 1].\n",
    "    ufunc = lambda first, _: first >= target_char_idx_start and first < target_char_idx_end\n",
    "    y_pred_mask = [ufunc(*offset) for offset in token_offsets]\n",
    "    try:\n",
    "        y_first = 3 + y_pred_mask.index(True)\n",
    "        y_last = 3 + len(y_pred_mask) - y_pred_mask[::-1].index(True) - 1\n",
    "    except ValueError:\n",
    "        # some of the labels are noisy, and the first character in the label does not actually\n",
    "        # correspond with the first character of any token (e.g. the label is a part-of-a-word\n",
    "        # instead of a word). I'm going to venture the opinion here that these records \n",
    "        # constitute data noise (because, I mean, they are) and should be removed in\n",
    "        # pre-processing\n",
    "        raise ValueError(\n",
    "            f\"Found bad selected_text value '{selected_text}' for tweet '{tweet}'.\"\n",
    "            f\"Make sure to get rid of these in a pre-processing pass.\"\n",
    "        )\n",
    "    \n",
    "    # convert to torch tensors\n",
    "    t = lambda seq: torch.tensor(seq, dtype=torch.long)\n",
    "    input_ids, token_type_ids, attention_mask, y_first, y_last =\\\n",
    "        t(input_ids), t(token_type_ids), t(attention_mask), t(y_first), t(y_last)\n",
    "    \n",
    "    # output\n",
    "    # Unfortunately the PyTorch dataloader relies on pickle, and TIL namedtuples do not play nice\n",
    "    # with pickle!\n",
    "    # Record = namedtuple('record', 'input_ids token_type_ids attention_mask y_first y_last')\n",
    "    # return Record(input_ids, token_type_ids, attention_mask, y_first, y_last)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"y_first\": y_first,\n",
    "        \"y_last\": y_last\n",
    "    }\n",
    "\n",
    "class TwitterSentimentExtractionDataset:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizers.BertWordPieceTokenizer(\n",
    "            f\"/mnt/bert-base-uncased/vocab.txt\", lowercase=True\n",
    "        )\n",
    "        self.max_len = 128\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return process_data(\n",
    "            self.df.text[item],\n",
    "            self.df.selected_text[item], \n",
    "            self.df.sentiment[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "        )\n",
    "\n",
    "# preprocessing pass; see comments in the previous code cell on why this is necessary\n",
    "X_train_preprocessing_pass = TwitterSentimentExtractionDataset(train)\n",
    "\n",
    "bad_idxs, good_idxs, y_firsts, y_lasts = [], [], [], []\n",
    "for i in range(len(train)):\n",
    "# for i in tqdm.tqdm(list(range(len(train)))):\n",
    "    try:\n",
    "        x = X_train_preprocessing_pass[i]\n",
    "        y_firsts.append(x['y_first'])\n",
    "        y_lasts.append(x['y_last'])\n",
    "        good_idxs.append(i)\n",
    "    except ValueError:\n",
    "        print(f\"Found bad record at idx {i}.\")\n",
    "        y_firsts.append(None)\n",
    "        y_lasts.append(None)\n",
    "        bad_idxs.append(i)\n",
    "\n",
    "del X_train_preprocessing_pass\n",
    "train_orig = train\n",
    "X_train_df = train.iloc[good_idxs].reset_index(drop=True)\n",
    "\n",
    "class TwitterSentimentExtractionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # configuring the model to output hidden states\n",
    "        cfg = transformers.PretrainedConfig.get_config_dict(\"bert-base-uncased\")[0]  # tuple?\n",
    "        cfg[\"output_hidden_states\"] = True\n",
    "        cfg[\"gradient_checkpointing\"] = True\n",
    "        cfg = transformers.BertConfig.from_dict(cfg)\n",
    "        self.bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\", config=cfg)\n",
    "        \n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.l0 = nn.Linear(768 * 2, 2)\n",
    "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
    "        # self.out = nn.LogSoftmax(dim=-2)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # ignore the output from the model head, we'll instead we'll construct our own attention\n",
    "        # head connected to the last two layers of hidden weights.\n",
    "        # that's 512x762x2=780288 connections.\n",
    "        _, _, out = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        out = torch.cat((out[-1], out[-2]), dim=-1)\n",
    "        out = self.drop_out(out)\n",
    "        # the new head uses a linear layer with two output nodes.\n",
    "        # the first node learns sequence start.\n",
    "        # the second node learns sequence end.\n",
    "        logits = self.l0(out)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        \n",
    "        return start_logits, end_logits\n",
    "        \n",
    "        # TODO: embed softmax directly into the model arch\n",
    "        # y_start, y_end = self.out(start_logits), self.out(end_logits)\n",
    "        # return y_start, y_end\n",
    "\n",
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    y_pred_first_loss = ce(start_logits, start_positions)\n",
    "    y_pred_last_loss = ce(end_logits, end_positions)\n",
    "    y_pred_loss = (y_pred_first_loss + y_pred_last_loss)\n",
    "    return y_pred_loss\n",
    "\n",
    "# create model\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = TwitterSentimentExtractionModel()\n",
    "model.to(device)\n",
    "\n",
    "# create optimizer; uses weight decay\n",
    "model_parameters = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "    {'params': [p for n, p in model_parameters if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "    {'params': [p for n, p in model_parameters if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "\n",
    "# batch size and epochs globals; setting batch_size = 1 for testing, 64 is too much for the K80\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "# dataset and dataloader\n",
    "dataset = TwitterSentimentExtractionDataset(X_train_df)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "# create scheduler, this one is a transformers module hookup, thanks huggingface\n",
    "num_train_steps = int(len(dataloader) * epochs)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_train_steps\n",
    ")\n",
    "\n",
    "# tensorboard writer\n",
    "writer = SummaryWriter(f'/spell/tensorboards/model_1')\n",
    "\n",
    "# train func for one epoch of training\n",
    "def train_fn(dataloader, model, optimizer, device, scheduler, epoch_num):\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    fn = lambda field: records[field].to(device, dtype=torch.long)\n",
    "    for idx, records in enumerate(dataloader):\n",
    "        # move the record to GPU\n",
    "        input_ids = fn(\"input_ids\")\n",
    "        token_type_ids = fn(\"token_type_ids\")\n",
    "        attention_mask = fn(\"attention_mask\")\n",
    "        y_first = fn(\"y_first\")\n",
    "        y_last = fn(\"y_last\")\n",
    "        \n",
    "        model.zero_grad()\n",
    "        y_pred_start_logits, y_pred_end_logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        loss = loss_fn(y_pred_start_logits, y_pred_end_logits, y_first, y_last)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        y_pred_starts = torch.softmax(y_pred_start_logits, dim=1).cpu().detach().numpy()\n",
    "        y_pred_ends = torch.softmax(y_pred_end_logits, dim=1).cpu().detach().numpy()\n",
    "        lv = loss.item()\n",
    "        losses.append(lv)\n",
    "        writer.add_scalar('training loss', lv, epoch_num * len(dataloader) + idx)\n",
    "        \n",
    "        if idx % 10 == 0:\n",
    "            print(f\"epoch {epoch_num}, batch {idx} training loss: {losses[-1]}\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "def main():\n",
    "    checkpoints_dir = \"/spell/checkpoints/\"\n",
    "    if not os.path.exists(checkpoints_dir):\n",
    "        os.mkdir(checkpoints_dir)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        losses = train_fn(dataloader, model, optimizer, device, scheduler, epoch)\n",
    "        avg_loss = np.mean(losses)\n",
    "        print(f\"epoch {epoch}, average training loss: {avg_loss}\")\n",
    "        torch.save(model.state_dict(), f\"/spell/checkpoints/model_{epoch}.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ]
}